{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from env.utility.helps import Bob\n",
    "from env.utility.file_management import File_Management\n",
    "from env.audit import Audits\n",
    "from croniter import croniter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.modules.activity import main as Activity\n",
    "from env.modules.apps import main as Apps\n",
    "from env.modules.catalog import main as Catalog\n",
    "from env.modules.graph import main as Graph\n",
    "from env.modules.tenant import main as Tenant\n",
    "from env.modules.refreshhistory import main as RefreshHistory\n",
    "from env.modules.refreshables import main as Refreshables\n",
    "from env.modules.gateway import main as Gateway\n",
    "from env.modules.capacity import main as Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "keyvault_url = \"https://powerbimonitorkv.vault.azure.net/\"\n",
    "\n",
    "# Create an instance of DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Create a SecretClient instance\n",
    "client = SecretClient(vault_url=keyvault_url, credential=credential)\n",
    "\n",
    "# Use the client to access secrets in Key Vault\n",
    "# keyvault_url = \"https://powerbimonitorkv.vault.azure.net/\"\n",
    "\n",
    "# from notebook.utils import mssparkutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenant ID: 0b69ab40-1bc7-4666-9f20-691ba105a907\n",
      "Client ID: e15fadf9-bc05-4ade-af9f-d79a918bbacd\n",
      "Client Secret: gCa8Q~w9zuEnkl9SBCo7OXcwriGBA7C26nGTIdzs\n",
      "Workspace Name: FabricMonitor\n",
      "Task Activity is now running\n",
      "try again\n",
      "An exception occurred while reading the file: 'NoneType' object has no attribute 'get_ServicePrincipal'\n",
      "\n",
      "Total elapsed time: 0.1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tenant_id' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m audit\u001b[38;5;241m.\u001b[39mset_Activity_cron(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m* * * * * 30\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(audit.ServicePrincipal)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m audit\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/projects/fabric/monitor/admin/env/audit.py:163\u001b[0m, in \u001b[0;36mAudits.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m     gather_tasks\u001b[38;5;241m.\u001b[39mappend(asyncio\u001b[38;5;241m.\u001b[39mcreate_task(task(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, work_queue, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext)))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Timer(text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal elapsed time: \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mgather_tasks)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# this has all the information needed to modify the state.yaml file\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# update the state.yaml file with the last run information\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_state, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/projects/fabric/monitor/admin/env/audit.py:152\u001b[0m, in \u001b[0;36mAudits.run.<locals>.task\u001b[0;34m(name, work_queue, content)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremove_carriage_returns(module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is now running\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    151\u001b[0m timer\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m module(content)\n\u001b[1;32m    153\u001b[0m timer\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/projects/fabric/monitor/admin/env/modules/activity.py:112\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m activity_events(url\u001b[38;5;241m=\u001b[39mrest_api, headers\u001b[38;5;241m=\u001b[39mheaders, pivotDate\u001b[38;5;241m=\u001b[39mpivotDate)                        \n\u001b[1;32m    110\u001b[0m         pivotDate \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m get_activity()\n",
      "File \u001b[0;32m~/projects/fabric/monitor/admin/env/modules/activity.py:96\u001b[0m, in \u001b[0;36mmain.<locals>.get_activity\u001b[0;34m(pivotDate)\u001b[0m\n\u001b[1;32m     93\u001b[0m pageIndex \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m flagNoActivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpbi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# keep the start and end time within a 24 hour period by adding 24 hours and removing 1 second \u001b[39;00m\n\u001b[1;32m     99\u001b[0m nextDate \u001b[38;5;241m=\u001b[39m (pivotDate \u001b[38;5;241m+\u001b[39m timedelta(hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)) \u001b[38;5;241m+\u001b[39m timedelta(seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/projects/fabric/monitor/admin/env/clients/baseClient.py:29\u001b[0m, in \u001b[0;36mClient.get_headers\u001b[0;34m(self, graph, tenant)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception occurred while reading the file:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m---> 29\u001b[0m authority \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://login.microsoftonline.com/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtenant_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# if graph:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     authority = f\"https://login.microsoftonline.com/{tenant_id}\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#     scope = \"https://graph.microsoft.com/.default\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Create a ConfidentialClientApplication object\u001b[39;00m\n\u001b[1;32m     43\u001b[0m app \u001b[38;5;241m=\u001b[39m msal\u001b[38;5;241m.\u001b[39mConfidentialClientApplication(\n\u001b[1;32m     44\u001b[0m     client_id\u001b[38;5;241m=\u001b[39mclient_id,\n\u001b[1;32m     45\u001b[0m     client_credential\u001b[38;5;241m=\u001b[39mclient_secret,\n\u001b[1;32m     46\u001b[0m     authority\u001b[38;5;241m=\u001b[39mauthority\n\u001b[1;32m     47\u001b[0m )\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tenant_id' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from env.audit import Audits\n",
    "\n",
    "appsecret = os.getenv(\"admin_appsecret\")\n",
    "appid = os.getenv(\"admin_appid\")\n",
    "tenantid = os.getenv(\"admin_tenantid\")\n",
    "\n",
    "audit = Audits()\n",
    "audit.set_ServicePrincipal(\n",
    "    tenant_id=tenantid,\n",
    "    client_id=appid,\n",
    "    client_secret= appsecret\n",
    ")\n",
    "\n",
    "# audit.set_ApplicationModules(\"Activity,Apps,Capacity,Catalog,Domains,FabricItems,Gateway,Graph,Refreshables,RefreshHistory,Roles,Tenant,Workspaces\")\n",
    "audit.set_ApplicationModules(\"Activity\")\n",
    "\n",
    "audit.set_LakehouseName(\"FabricLake\")\n",
    "audit.set_PathInLakehouse(\"stage\")\n",
    "audit.set_WorkspaceName(\"FabricMonitor\")\n",
    "\n",
    "audit.set_Activity_cron(\"* * * * * 30\")\n",
    "\n",
    "# print(audit.ServicePrincipal)\n",
    "await audit.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit.set_Catalog_cron(\"* * * * * 30\")\n",
    "audit.set_Capacity_cron(\"* * * * * 30\")\n",
    "audit.set_Apps_cron(\"* * * * * 30\")\n",
    "audit.set_Domains_cron(\"* * * * * 30\") # Not yet implemented\n",
    "audit.set_Graph_cron(\"* * * * * 30\")\n",
    "audit.set_Tenant_cron(\"* * * * * 30\")\n",
    "audit.set_RefreshHistory_cron(\"* * * * * 30\")\n",
    "audit.set_Refreshables_cron(\"* * * * * 30\")\n",
    "audit.set_Gateway_cron(\"* * * * * 30\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '{\"Domains\":{\"lastRun\":\"today\"}}'\n",
    "import json\n",
    "j = json.loads(t)\n",
    "print(j)\n",
    "\n",
    "j = t.get(\"Domainss\",{}).get(\"lastRun\")\n",
    "if j:\n",
    "    print(\"found\")\n",
    "else:\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.audit import Audits\n",
    "\n",
    "audit = Audits()  # Await the coroutine object\n",
    "audit.LakehouseName = \"FabricLakehouse\"\n",
    "\n",
    "# audit.ServicePrincipal[\"AppId\"] = \"a4f7b4d5-7d0e-4b0b-a5b1-4e2b8c8f6e2e\"\n",
    "from env.utility.helps import Bob\n",
    "bob = Bob()\n",
    "bob.audit = audit\n",
    "\n",
    "bob.audit.LakehouseName = \"FabricLakeHome\"\n",
    "\n",
    "print(bob.audit.LakehouseName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scheduler(cron_time, last_run=None):\n",
    "\n",
    "    year = datetime.now().year\n",
    "    month = datetime.now().month\n",
    "    day = datetime.now().day\n",
    "\n",
    "    local_date = datetime(year, month, day, tzinfo=timezone.utc)\n",
    "    val = croniter(cron_time, local_date).get_next(datetime)\n",
    "    last_run = bob.convert_dt_str(last_run)\n",
    "    print(f\"did i even know this value {last_run}\")\n",
    "\n",
    "    # Determine if the current day of the week is the same as the value for day of the week in the cron value\n",
    "    current_day_of_week = datetime.now().strftime(\"%A\")\n",
    "    cron_day_of_week = cron_time.split(\" \")[4]\n",
    "\n",
    "\n",
    "    # Get the next scheduled datetime\n",
    "    next_datetime = val\n",
    "    print(next_datetime)\n",
    "\n",
    "    # Check if the current datetime matches the next scheduled datetime\n",
    "    if datetime.now().strftime(\"%Y-%m-%d\") == next_datetime.strftime(\"%Y-%m-%d\"):\n",
    "        print(\"Hello World, I am supposed to run today\")\n",
    "    else:\n",
    "        print(\"Goodnight World, I am sleeping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_function_due(cron_syntax, last_run):\n",
    "    # last_run_datetime = last_run\n",
    "    now = datetime.now()\n",
    "    cron = croniter(cron_syntax, now)\n",
    "    \n",
    "    next_run_datetime = cron.get_next(datetime)\n",
    "\n",
    "    if next_run_datetime.strftime(\"%Y-%m-%d\") <= datetime.now().strftime(\"%Y-%m-%d\"):\n",
    "        return {\"IsDue\": True, \"NextRun\": next_run_datetime, \"LastRun\": last_run}\n",
    "    else:\n",
    "        return {\"IsDue\": False, \"NextRun\": next_run_datetime, \"LastRun\": last_run}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cron(sched='0 4 * * 6'):\n",
    "    now = datetime.now()\n",
    "    cron = croniter(sched, now)\n",
    "\n",
    "    for i in range(8):\n",
    "        nextdate = cron.get_next(datetime)\n",
    "        print (nextdate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cron(\"0 0 * * 0-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "modules = settings.get(\"ApplicationModules\").replace(\" \",\"\").split(\",\")\n",
    "# classes = [globals()[module] for module in modules]\n",
    "\n",
    "classes = [\"Activity\"]\n",
    "\n",
    "\n",
    "for module in classes:\n",
    "    sched = settings.get(f\"{module}_cron\")\n",
    "    run = state.get(f\"{module.lower()}\").get(\"lastRun\")\n",
    "    print(sched)\n",
    "    last_run = datetime.now()#  bob.convert_dt_str(run)    \n",
    "\n",
    "    results = is_function_due(sched,last_run)\n",
    "    r = test_cron(sched)\n",
    "\n",
    "    if results.get(\"IsDue\"):\n",
    "        print(f\"Function {module} last ran on {run} and is due to run at {results.get('NextRun')} compared to {r}\")\n",
    "    else:\n",
    "        print(f\"Function {module} last ran on {run} and is not due to run but will run after {results.get('NextRun')} compared to {r}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.get(\"Refreshables_cron\")\n",
    "state.get(\"refreshables\").get(\"lastRun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.get('activity').get('lastRun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.powerbi.com/v1.0/myorg/gateways'\n",
    "response = requests.get(url=api_url, headers=headers)\n",
    "results = response.json()\n",
    "print(response.status_code)\n",
    "gateways = list()\n",
    "for gateway in results['value']:\n",
    "    gateways.append(gateway.get(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list()\n",
    "for id in gateways:\n",
    "    response = requests.get(f'https://api.powerbi.com/v1.0/myorg/gateways/{id}/datasources', headers=headers)\n",
    "    doc_results = response.json()\n",
    "    r.append(doc_results['value'])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateways = list()\n",
    "\n",
    "for i in range(0, len(r)):\n",
    "    if isinstance(r[i], list):\n",
    "        for j in range(0, len(r[i])):\n",
    "            gateways.append(r[i][j])\n",
    "    else:\n",
    "        gateways.append(r[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list()\n",
    "\n",
    "for i in range(0,len(gateways)):\n",
    "    api_users = f'https://api.powerbi.com/v1.0/myorg/gateways/{gateways[i][\"gatewayId\"]}/datasources/{gateways[i][\"id\"]}/users'\n",
    "    response = requests.get(api_users, headers=headers)\n",
    "    results = response.json()\n",
    "    results['value'][0]['datasourceId'] = gateways[i][\"id\"]\n",
    "    results['value'][0]['gatewayId'] = gateways[i][\"gatewayId\"]\n",
    "    users.append(results['value'][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasource Connectivity  \n",
    "\n",
    "This only reports back connectivity to the datasource as an error. Also, you need to use the response.text as the JSON is not populated. A successful connection will report back as HTTP 200 while a failed connection will result in HTTP 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will be used for a template dictionary for a successful connection to the datasource\n",
    "status_base = {   \n",
    "    \"datasourceId\":\"\",\n",
    "    \"gatewayId\":\"\",\n",
    "    \"error\":\n",
    "    {\n",
    "        \"code\":\"success\",\n",
    "        \"pbi.error\":{ \n",
    "            \"code\":\"0\",\n",
    "            \"parameters\":{},\n",
    "            \"details\":[],\n",
    "            \"exceptionCulprit\":0\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status  = list()\n",
    "for i in range(0,len(gateways)):\n",
    "    api_status = f'https://api.powerbi.com/v1.0/myorg/gateways/{gateways[i][\"gatewayId\"]}/datasources/{gateways[i][\"id\"]}/status'\n",
    "    \n",
    "    response = requests.get(api_status, headers=headers)\n",
    "    if response.ok:\n",
    "        # response.json() is actually empty when it is a success\n",
    "        \n",
    "        status_base['datasourceId'] = gateways[i][\"id\"]\n",
    "        status_base['gatewayId'] = gateways[i][\"gatewayId\"]\n",
    "        status_base.get(\"error\").get(\"pbi.error\").get(\"details\").append({\"message\":\"success\",\"detail\":response.status_code})    \n",
    "        status.append(status_base)\n",
    "    else:\n",
    "        results = json.loads(response.text)\n",
    "        results[\"datasourceId\"] = gateways[i][\"id\"]\n",
    "        results[\"gatewayId\"] = gateways[i][\"gatewayId\"]\n",
    "        results.get(\"error\").get(\"pbi.error\").get(\"details\").append({\"message\":\"cannot connect\",\"detail\":response.status_code})    \n",
    "        status.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = list()\n",
    "\n",
    "for i in range(0,len(gateways)):\n",
    "    api_datasource = f'https://api.powerbi.com/v1.0/myorg/gateways/{gateways[i][\"gatewayId\"]}/datasources/{gateways[i][\"id\"]}'\n",
    "    response = requests.get(api_datasource, headers=headers)\n",
    "    results = response.json()\n",
    "    results.pop('@odata.context')\n",
    "    datasources.append(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "sender_email = 'brcampb@microsoft.com'\n",
    "receiver_email = 'brandonh.campbell@gmail.com'\n",
    "subject = 'Hello from Python!'\n",
    "body = 'This is a test email sent from a Python script.'\n",
    "\n",
    "msg = MIMEText(body)\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = receiver_email\n",
    "\n",
    "# Set up the SMTP server (e.g., Gmail)\n",
    "with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
    "    server.login(sender_email, 'your_password')\n",
    "    server.sendmail(sender_email, [receiver_email], msg.as_string())\n",
    "\n",
    "\n",
    "\n",
    "sender_email = 'your_email@example.com'\n",
    "receiver_email = 'recipient_email@example.com'\n",
    "subject = 'Hello from Python!'\n",
    "body = 'This is a test email sent from a Python script.'\n",
    "\n",
    "msg = MIMEText(body)\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = receiver_email\n",
    "\n",
    "# Set up the SMTP server (e.g., Outlook)\n",
    "with smtplib.SMTP('smtp.office365.com', 587) as server:\n",
    "    server.starttls()\n",
    "    server.login(sender_email, 'your_password')\n",
    "    server.sendmail(sender_email, [receiver_email], msg.as_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Set up the SMTP server\n",
    "smtp_server = 'smtp.gmail.com'\n",
    "smtp_port = 587\n",
    "sender_email = 'your_email@example.com'\n",
    "password = 'your_password'\n",
    "\n",
    "# Create a message\n",
    "subject = 'Hello from Python!'\n",
    "body = 'This is a test email sent from a Python script.'\n",
    "msg = MIMEText(body)\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = 'recipient_email@example.com'\n",
    "\n",
    "# Send the email\n",
    "with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
    "    server.starttls()\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(sender_email, [msg['To']], msg.as_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest_api = \"admin/capacities/refreshables\"\n",
    "# GET https://api.powerbi.com/v1.0/myorg/admin/groups?$expand=datasets\n",
    "# htpps://api.powerbi.com/v1.0/myorg/admin/groups?$expand=datasets\n",
    "rest_api = \"https://api.powerbi.com/v1.0/myorg/admin/groups?$expand=datasets&$top=5000\"\n",
    "\n",
    "response = requests.get(url=rest_api, headers=headers)\n",
    "workspaces = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in workspaces['value']:\n",
    "    group_id = item['id']\n",
    "    for dataset in item['datasets']:\n",
    "        dataset_id = dataset['id']\n",
    "        print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET https://api.powerbi.com/v1.0/myorg/groups/{groupId}/datasets/{datasetId}/refreshes\n",
    "refresh_history = list()\n",
    "\n",
    "for item in workspaces['value']:\n",
    "    group_id = item['id']\n",
    "    for dataset in item['datasets']:\n",
    "        dataset_id = dataset['id']\n",
    "        if dataset['isRefreshable']==True and dataset['addRowsAPIEnabled']==False:\n",
    "            rest_api = f\"https://api.powerbi.com/v1.0/myorg/groups/{group_id}/datasets/{dataset_id}/refreshes\"\n",
    "\n",
    "            response = requests.get(url=rest_api, headers=headers)\n",
    "            if response.ok:\n",
    "                results = response.json()\n",
    "                for result in results['value']:\n",
    "                    if len(result)>0:\n",
    "                        refresh_history.append(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fd = pd.DataFrame(refresh_history)\n",
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[fd['requestId']=='9cbca636-ec64-4ff6-9dfe-9e2e5acc3056']['refreshAttempts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize(df:pd.DataFrame(), base2:pd.DataFrame(), json_column='workspace_id'):\n",
    "    for row in df.itertuples():\n",
    "        column_number = df.columns.get_loc(json_column)+1\n",
    "        if isinstance(row[column_number], list) and len(row[column_number])>0:\n",
    "            for item in row[column_number]:\n",
    "                # item[id_name] = row.id\n",
    "                base2 = pd.concat([base2, df, pd.json_normalize(item)])\n",
    "                base2.drop(json_column, axis=1, inplace=True)\n",
    "        else:\n",
    "            if isinstance(row[column_number], dict):\n",
    "                print(\"Yes it is a dictionary\")\n",
    "                base2 = pd.concat([base2, df, pd.json_normalize(row[column_number])])\n",
    "                base2.drop(json_column, axis=1, inplace=True)\n",
    "    return base2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = normalize(fd, pd.DataFrame(), 'refreshAttempts')\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = normalize(dd, pd.DataFrame(), 'serviceExceptionJson')\n",
    "de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "string_dict = \"{'id': '8271b9f0-3ff7-46b5-9f8a-5a3ba59d01b7', 'name': 'FabricMonitor', 'addRowsAPIEnabled': False, 'configuredBy': 'brandon.campbell@mngenvmcap084084.onmicrosoft.com', 'isRefreshable': True, 'isEffectiveIdentityRequired': False, 'isEffectiveIdentityRolesRequired': False, 'targetStorageMode': 'Abf', 'createdDate': '2024-03-27T18:42:49.663Z', 'contentProviderType': 'PbixInImportMode', 'upstreamDatasets': [], 'users': [], 'isInPlaceSharingEnabled': False}\"\n",
    "\n",
    "\n",
    "\n",
    "print(dict_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dd.itertuples():\n",
    "    if isinstance(row.serviceExceptionJson, list) and len(row.serviceExceptionJson)>0:\n",
    "        for item in row.serviceExceptionJson:\n",
    "            print(item)\n",
    "    else:\n",
    "        if isinstance(row.serviceExceptionJson, str):\n",
    "            print(\"Yes it is a dictionary\")\n",
    "            print(row.serviceExceptionJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=\"https://api.powerbi.com/v1.0/myorg/admin/capacities/refreshables?$top=5000\"\n",
    "\n",
    "refreshables = list()\n",
    "\n",
    "response = requests.get(url=api, headers=headers)\n",
    "if response.ok:\n",
    "    results = response.json()\n",
    "    for result in results['value']:\n",
    "        refreshables.append(result)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(refresh_history)>=len(refreshables):\n",
    "    print(\"there is more information in refresh_history than in refreshables\")\n",
    "else:\n",
    "    print(\"there is more information in refreshables than in refresh_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(refreshables)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.json_normalize(refresh_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('refreshables.csv', index=False)\n",
    "df2.to_csv('refresh_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[['requestId','refreshAttempts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the DataFrame with the column 'refreshAttempts' is named 'df'\n",
    "df_expanded = pd.json_normalize(df3['refreshAttempts'])\n",
    "\n",
    "# Merge the expanded DataFrame with the original DataFrame\n",
    "df_merged = pd.concat([df3.drop('refreshAttempts', axis=1), df_expanded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivoted = df_merged.melt(id_vars=['requestId'], var_name='column', value_name='value')\n",
    "df_unpivoted.rename(columns={'requestId':'requestId','column':'column','value':'refreshAttempts'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Convert JSON column to DataFrame\n",
    "df_refreshAttempts = pd.json_normalize(df_unpivoted['refreshAttempts'])\n",
    "\n",
    "# Merge the original DataFrame with the new DataFrame\n",
    "df_merged = pd.concat([df_unpivoted.drop('refreshAttempts', axis=1), df_refreshAttempts], axis=1)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_clean = df_merged[['requestId','attemptId','startTime','endTime','serviceExceptionJson']]\n",
    "refresh_history_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_cleanest = df2.merge(refresh_history_clean, on='requestId', how='left')\n",
    "refresh_history_cleanest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_cleanest.drop(columns=['refreshAttempts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_cleanest.to_csv('refresh_history_cleanest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['--base','true','--fullscan','30']\n",
    "for i in range(0, len(args), 2):\n",
    "    print(args[i])\n",
    "    # if args[i] == '--base':\n",
    "    #     print(args[i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.fabric.microsoft.com/v1/admin/domains\", headers=headers)\n",
    "if response.ok:\n",
    "    print(response.json())\n",
    "    v = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = v.get(\"domains\")\n",
    "for domain in domains:\n",
    "    domainId = domain['id']\n",
    "\n",
    "    response = requests.get(f\"https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/workspaces\", headers=headers)\n",
    "    if response.ok:\n",
    "        print(response.json())\n",
    "    else:\n",
    "        print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.powerbi.com/v1.0/myorg/admin/capacities\"\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.ok:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.fabric.microsoft.com/v1/admin/workspaces\", headers=headers)\n",
    "if response.ok:\n",
    "    results = response.json()\n",
    "\n",
    "workspace = results.get(\"workspaces\")\n",
    "\n",
    "items = list()\n",
    "\n",
    "for item in workspace:\n",
    "    items.append(item[\"id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "url = f\"https://api.powerbi.com/v1.0/myorg/admin/groups/ef61e238-9402-4a64-9107-1e10f5d0fcc4?$expand=users\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.ok:\n",
    "    results = response.json()\n",
    "    print(results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items = set(items)\n",
    "workspace_lst = list()\n",
    "\n",
    "ceiling = len(items)\n",
    "print(ceiling)\n",
    "cnt = 0\n",
    "for item in items:\n",
    "    cnt+=1\n",
    "\n",
    "    if len(workspace_lst) > ceiling:\n",
    "        break\n",
    "\n",
    "    if cnt <= ceiling:\n",
    "        url = f\"https://api.powerbi.com/v1.0/myorg/admin/groups/{item}?$expand=users\"\n",
    "    \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.ok:\n",
    "            results = response.json()\n",
    "            workspace_lst.append(results)\n",
    "        else:\n",
    "            if response.status_code==429:\n",
    "                result = response.json()\n",
    "                print(f\"you must wait { int(result.get('message').split('.')[1].split(' ')[3])/60} minutes\")\n",
    "\n",
    "                break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(workspace_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_json(json.dumps(workspace_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\"message\": \"You have exceeded the amount of requests allowed in the current time frame and further requests will fail. Retry in 1223 seconds.\"}\n",
    "message = response.get(\"message\")\n",
    "\n",
    "tm = message.split(sep=\".\")[1].split(sep=\" \")[3]\n",
    "\n",
    "print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
