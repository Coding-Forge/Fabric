{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from env.utility.helps import Bob\n",
    "from env.utility.file_management import File_Management\n",
    "from env.audit import Audits\n",
    "from croniter import croniter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.modules.activity import main as Activity\n",
    "from env.modules.apps import main as Apps\n",
    "from env.modules.catalog import main as Catalog\n",
    "from env.modules.graph import main as Graph\n",
    "from env.modules.tenant import main as Tenant\n",
    "from env.modules.refreshhistory import main as RefreshHistory\n",
    "from env.modules.refreshables import main as Refreshables\n",
    "from env.modules.gateway import main as Gateway\n",
    "from env.modules.capacity import main as Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "keyvault_url = \"https://powerbimonitorkv.vault.azure.net/\"\n",
    "\n",
    "# Create an instance of DefaultAzureCredential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Create a SecretClient instance\n",
    "client = SecretClient(vault_url=keyvault_url, credential=credential)\n",
    "\n",
    "# Use the client to access secrets in Key Vault\n",
    "# keyvault_url = \"https://powerbimonitorkv.vault.azure.net/\"\n",
    "\n",
    "# from notebook.utils import mssparkutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world...I updated my repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from env.audit import Audits\n",
    "\n",
    "appsecret = os.getenv(\"admin_appsecret\")\n",
    "appid = os.getenv(\"admin_appid\")\n",
    "tenantid = os.getenv(\"admin_tenantid\")\n",
    "\n",
    "audit = Audits()\n",
    "audit.set_ServicePrincipal(\n",
    "    tenant_id=tenantid,\n",
    "    client_id=appid,\n",
    "    client_secret= appsecret\n",
    ")\n",
    "\n",
    "# audit.set_ApplicationModules(\"Activity,Apps,Capacity,Catalog,Domains,FabricItems,Gateway,Graph,Refreshables,RefreshHistory,Roles,Tenant,Workspaces\")\n",
    "audit.set_ApplicationModules(\"Activity\")\n",
    "\n",
    "audit.set_LakehouseName(\"FabricLake\")\n",
    "audit.set_PathInLakehouse(\"stage\")\n",
    "audit.set_WorkspaceName(\"FabricMonitor\")\n",
    "\n",
    "audit.set_Activity_cron(\"* * * * * 30\")\n",
    "\n",
    "# print(audit.ServicePrincipal)\n",
    "await audit.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit.set_Catalog_cron(\"* * * * * 30\")\n",
    "audit.set_Capacity_cron(\"* * * * * 30\")\n",
    "audit.set_Apps_cron(\"* * * * * 30\")\n",
    "audit.set_Domains_cron(\"* * * * * 30\") # Not yet implemented\n",
    "audit.set_Graph_cron(\"* * * * * 30\")\n",
    "audit.set_Tenant_cron(\"* * * * * 30\")\n",
    "audit.set_RefreshHistory_cron(\"* * * * * 30\")\n",
    "audit.set_Refreshables_cron(\"* * * * * 30\")\n",
    "audit.set_Gateway_cron(\"* * * * * 30\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = '{\"Domains\":{\"lastRun\":\"today\"}}'\n",
    "import json\n",
    "j = json.loads(t)\n",
    "print(j)\n",
    "\n",
    "j = t.get(\"Domainss\",{}).get(\"lastRun\")\n",
    "if j:\n",
    "    print(\"found\")\n",
    "else:\n",
    "    print(\"not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.audit import Audits\n",
    "\n",
    "audit = Audits()  # Await the coroutine object\n",
    "audit.LakehouseName = \"FabricLakehouse\"\n",
    "\n",
    "# audit.ServicePrincipal[\"AppId\"] = \"a4f7b4d5-7d0e-4b0b-a5b1-4e2b8c8f6e2e\"\n",
    "from env.utility.helps import Bob\n",
    "bob = Bob()\n",
    "bob.audit = audit\n",
    "\n",
    "bob.audit.LakehouseName = \"FabricLakeHome\"\n",
    "\n",
    "print(bob.audit.LakehouseName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scheduler(cron_time, last_run=None):\n",
    "\n",
    "    year = datetime.now().year\n",
    "    month = datetime.now().month\n",
    "    day = datetime.now().day\n",
    "\n",
    "    local_date = datetime(year, month, day, tzinfo=timezone.utc)\n",
    "    val = croniter(cron_time, local_date).get_next(datetime)\n",
    "    last_run = bob.convert_dt_str(last_run)\n",
    "    print(f\"did i even know this value {last_run}\")\n",
    "\n",
    "    # Determine if the current day of the week is the same as the value for day of the week in the cron value\n",
    "    current_day_of_week = datetime.now().strftime(\"%A\")\n",
    "    cron_day_of_week = cron_time.split(\" \")[4]\n",
    "\n",
    "\n",
    "    # Get the next scheduled datetime\n",
    "    next_datetime = val\n",
    "    print(next_datetime)\n",
    "\n",
    "    # Check if the current datetime matches the next scheduled datetime\n",
    "    if datetime.now().strftime(\"%Y-%m-%d\") == next_datetime.strftime(\"%Y-%m-%d\"):\n",
    "        print(\"Hello World, I am supposed to run today\")\n",
    "    else:\n",
    "        print(\"Goodnight World, I am sleeping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_function_due(cron_syntax, last_run):\n",
    "    # last_run_datetime = last_run\n",
    "    now = datetime.now()\n",
    "    cron = croniter(cron_syntax, now)\n",
    "    \n",
    "    next_run_datetime = cron.get_next(datetime)\n",
    "\n",
    "    if next_run_datetime.strftime(\"%Y-%m-%d\") <= datetime.now().strftime(\"%Y-%m-%d\"):\n",
    "        return {\"IsDue\": True, \"NextRun\": next_run_datetime, \"LastRun\": last_run}\n",
    "    else:\n",
    "        return {\"IsDue\": False, \"NextRun\": next_run_datetime, \"LastRun\": last_run}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cron(sched='0 4 * * 6'):\n",
    "    now = datetime.now()\n",
    "    cron = croniter(sched, now)\n",
    "\n",
    "    for i in range(8):\n",
    "        nextdate = cron.get_next(datetime)\n",
    "        print (nextdate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cron(\"0 0 * * 0-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "modules = settings.get(\"ApplicationModules\").replace(\" \",\"\").split(\",\")\n",
    "# classes = [globals()[module] for module in modules]\n",
    "\n",
    "classes = [\"Activity\"]\n",
    "\n",
    "\n",
    "for module in classes:\n",
    "    sched = settings.get(f\"{module}_cron\")\n",
    "    run = state.get(f\"{module.lower()}\").get(\"lastRun\")\n",
    "    print(sched)\n",
    "    last_run = datetime.now()#  bob.convert_dt_str(run)    \n",
    "\n",
    "    results = is_function_due(sched,last_run)\n",
    "    r = test_cron(sched)\n",
    "\n",
    "    if results.get(\"IsDue\"):\n",
    "        print(f\"Function {module} last ran on {run} and is due to run at {results.get('NextRun')} compared to {r}\")\n",
    "    else:\n",
    "        print(f\"Function {module} last ran on {run} and is not due to run but will run after {results.get('NextRun')} compared to {r}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.get(\"Refreshables_cron\")\n",
    "state.get(\"refreshables\").get(\"lastRun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.get('activity').get('lastRun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_url = 'https://api.powerbi.com/v1.0/myorg/gateways'\n",
    "response = requests.get(url=api_url, headers=headers)\n",
    "results = response.json()\n",
    "print(response.status_code)\n",
    "gateways = list()\n",
    "for gateway in results['value']:\n",
    "    gateways.append(gateway.get(\"id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list()\n",
    "for id in gateways:\n",
    "    response = requests.get(f'https://api.powerbi.com/v1.0/myorg/gateways/{id}/datasources', headers=headers)\n",
    "    doc_results = response.json()\n",
    "    r.append(doc_results['value'])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateways = list()\n",
    "\n",
    "for i in range(0, len(r)):\n",
    "    if isinstance(r[i], list):\n",
    "        for j in range(0, len(r[i])):\n",
    "            gateways.append(r[i][j])\n",
    "    else:\n",
    "        gateways.append(r[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list()\n",
    "\n",
    "for i in range(0,len(gateways)):\n",
    "    api_users = f'https://api.powerbi.com/v1.0/myorg/gateways/{gateways[i][\"gatewayId\"]}/datasources/{gateways[i][\"id\"]}/users'\n",
    "    response = requests.get(api_users, headers=headers)\n",
    "    results = response.json()\n",
    "    results['value'][0]['datasourceId'] = gateways[i][\"id\"]\n",
    "    results['value'][0]['gatewayId'] = gateways[i][\"gatewayId\"]\n",
    "    users.append(results['value'][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasource Connectivity  \n",
    "\n",
    "This only reports back connectivity to the datasource as an error. Also, you need to use the response.text as the JSON is not populated. A successful connection will report back as HTTP 200 while a failed connection will result in HTTP 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will be used for a template dictionary for a successful connection to the datasource\n",
    "status_base = {   \n",
    "    \"datasourceId\":\"\",\n",
    "    \"gatewayId\":\"\",\n",
    "    \"error\":\n",
    "    {\n",
    "        \"code\":\"success\",\n",
    "        \"pbi.error\":{ \n",
    "            \"code\":\"0\",\n",
    "            \"parameters\":{},\n",
    "            \"details\":[],\n",
    "            \"exceptionCulprit\":0\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status  = list()\n",
    "for i in range(0,len(gateways)):\n",
    "    api_status = f'https://api.powerbi.com/v1.0/myorg/gateways/{gateways[i][\"gatewayId\"]}/datasources/{gateways[i][\"id\"]}/status'\n",
    "    \n",
    "    response = requests.get(api_status, headers=headers)\n",
    "    if response.ok:\n",
    "        # response.json() is actually empty when it is a success\n",
    "        \n",
    "        status_base['datasourceId'] = gateways[i][\"id\"]\n",
    "        status_base['gatewayId'] = gateways[i][\"gatewayId\"]\n",
    "        status_base.get(\"error\").get(\"pbi.error\").get(\"details\").append({\"message\":\"success\",\"detail\":response.status_code})    \n",
    "        status.append(status_base)\n",
    "    else:\n",
    "        results = json.loads(response.text)\n",
    "        results[\"datasourceId\"] = gateways[i][\"id\"]\n",
    "        results[\"gatewayId\"] = gateways[i][\"gatewayId\"]\n",
    "        results.get(\"error\").get(\"pbi.error\").get(\"details\").append({\"message\":\"cannot connect\",\"detail\":response.status_code})    \n",
    "        status.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = list()\n",
    "\n",
    "for i in range(0,len(gateways)):\n",
    "    api_datasource = f'https://api.powerbi.com/v1.0/myorg/gateways/{gateways[i][\"gatewayId\"]}/datasources/{gateways[i][\"id\"]}'\n",
    "    response = requests.get(api_datasource, headers=headers)\n",
    "    results = response.json()\n",
    "    results.pop('@odata.context')\n",
    "    datasources.append(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "sender_email = 'brcampb@microsoft.com'\n",
    "receiver_email = 'brandonh.campbell@gmail.com'\n",
    "subject = 'Hello from Python!'\n",
    "body = 'This is a test email sent from a Python script.'\n",
    "\n",
    "msg = MIMEText(body)\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = receiver_email\n",
    "\n",
    "# Set up the SMTP server (e.g., Gmail)\n",
    "with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
    "    server.login(sender_email, 'your_password')\n",
    "    server.sendmail(sender_email, [receiver_email], msg.as_string())\n",
    "\n",
    "\n",
    "\n",
    "sender_email = 'your_email@example.com'\n",
    "receiver_email = 'recipient_email@example.com'\n",
    "subject = 'Hello from Python!'\n",
    "body = 'This is a test email sent from a Python script.'\n",
    "\n",
    "msg = MIMEText(body)\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = receiver_email\n",
    "\n",
    "# Set up the SMTP server (e.g., Outlook)\n",
    "with smtplib.SMTP('smtp.office365.com', 587) as server:\n",
    "    server.starttls()\n",
    "    server.login(sender_email, 'your_password')\n",
    "    server.sendmail(sender_email, [receiver_email], msg.as_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Set up the SMTP server\n",
    "smtp_server = 'smtp.gmail.com'\n",
    "smtp_port = 587\n",
    "sender_email = 'your_email@example.com'\n",
    "password = 'your_password'\n",
    "\n",
    "# Create a message\n",
    "subject = 'Hello from Python!'\n",
    "body = 'This is a test email sent from a Python script.'\n",
    "msg = MIMEText(body)\n",
    "msg['Subject'] = subject\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = 'recipient_email@example.com'\n",
    "\n",
    "# Send the email\n",
    "with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
    "    server.starttls()\n",
    "    server.login(sender_email, password)\n",
    "    server.sendmail(sender_email, [msg['To']], msg.as_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest_api = \"admin/capacities/refreshables\"\n",
    "# GET https://api.powerbi.com/v1.0/myorg/admin/groups?$expand=datasets\n",
    "# htpps://api.powerbi.com/v1.0/myorg/admin/groups?$expand=datasets\n",
    "rest_api = \"https://api.powerbi.com/v1.0/myorg/admin/groups?$expand=datasets&$top=5000\"\n",
    "\n",
    "response = requests.get(url=rest_api, headers=headers)\n",
    "workspaces = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in workspaces['value']:\n",
    "    group_id = item['id']\n",
    "    for dataset in item['datasets']:\n",
    "        dataset_id = dataset['id']\n",
    "        print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET https://api.powerbi.com/v1.0/myorg/groups/{groupId}/datasets/{datasetId}/refreshes\n",
    "refresh_history = list()\n",
    "\n",
    "for item in workspaces['value']:\n",
    "    group_id = item['id']\n",
    "    for dataset in item['datasets']:\n",
    "        dataset_id = dataset['id']\n",
    "        if dataset['isRefreshable']==True and dataset['addRowsAPIEnabled']==False:\n",
    "            rest_api = f\"https://api.powerbi.com/v1.0/myorg/groups/{group_id}/datasets/{dataset_id}/refreshes\"\n",
    "\n",
    "            response = requests.get(url=rest_api, headers=headers)\n",
    "            if response.ok:\n",
    "                results = response.json()\n",
    "                for result in results['value']:\n",
    "                    if len(result)>0:\n",
    "                        refresh_history.append(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fd = pd.DataFrame(refresh_history)\n",
    "fd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd[fd['requestId']=='9cbca636-ec64-4ff6-9dfe-9e2e5acc3056']['refreshAttempts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize(df:pd.DataFrame(), base2:pd.DataFrame(), json_column='workspace_id'):\n",
    "    for row in df.itertuples():\n",
    "        column_number = df.columns.get_loc(json_column)+1\n",
    "        if isinstance(row[column_number], list) and len(row[column_number])>0:\n",
    "            for item in row[column_number]:\n",
    "                # item[id_name] = row.id\n",
    "                base2 = pd.concat([base2, df, pd.json_normalize(item)])\n",
    "                base2.drop(json_column, axis=1, inplace=True)\n",
    "        else:\n",
    "            if isinstance(row[column_number], dict):\n",
    "                print(\"Yes it is a dictionary\")\n",
    "                base2 = pd.concat([base2, df, pd.json_normalize(row[column_number])])\n",
    "                base2.drop(json_column, axis=1, inplace=True)\n",
    "    return base2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = normalize(fd, pd.DataFrame(), 'refreshAttempts')\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = normalize(dd, pd.DataFrame(), 'serviceExceptionJson')\n",
    "de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "string_dict = \"{'id': '8271b9f0-3ff7-46b5-9f8a-5a3ba59d01b7', 'name': 'FabricMonitor', 'addRowsAPIEnabled': False, 'configuredBy': 'brandon.campbell@mngenvmcap084084.onmicrosoft.com', 'isRefreshable': True, 'isEffectiveIdentityRequired': False, 'isEffectiveIdentityRolesRequired': False, 'targetStorageMode': 'Abf', 'createdDate': '2024-03-27T18:42:49.663Z', 'contentProviderType': 'PbixInImportMode', 'upstreamDatasets': [], 'users': [], 'isInPlaceSharingEnabled': False}\"\n",
    "\n",
    "\n",
    "\n",
    "print(dict_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in dd.itertuples():\n",
    "    if isinstance(row.serviceExceptionJson, list) and len(row.serviceExceptionJson)>0:\n",
    "        for item in row.serviceExceptionJson:\n",
    "            print(item)\n",
    "    else:\n",
    "        if isinstance(row.serviceExceptionJson, str):\n",
    "            print(\"Yes it is a dictionary\")\n",
    "            print(row.serviceExceptionJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=\"https://api.powerbi.com/v1.0/myorg/admin/capacities/refreshables?$top=5000\"\n",
    "\n",
    "refreshables = list()\n",
    "\n",
    "response = requests.get(url=api, headers=headers)\n",
    "if response.ok:\n",
    "    results = response.json()\n",
    "    for result in results['value']:\n",
    "        refreshables.append(result)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(refresh_history)>=len(refreshables):\n",
    "    print(\"there is more information in refresh_history than in refreshables\")\n",
    "else:\n",
    "    print(\"there is more information in refreshables than in refresh_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(refreshables)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.json_normalize(refresh_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('refreshables.csv', index=False)\n",
    "df2.to_csv('refresh_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[['requestId','refreshAttempts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the DataFrame with the column 'refreshAttempts' is named 'df'\n",
    "df_expanded = pd.json_normalize(df3['refreshAttempts'])\n",
    "\n",
    "# Merge the expanded DataFrame with the original DataFrame\n",
    "df_merged = pd.concat([df3.drop('refreshAttempts', axis=1), df_expanded], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivoted = df_merged.melt(id_vars=['requestId'], var_name='column', value_name='value')\n",
    "df_unpivoted.rename(columns={'requestId':'requestId','column':'column','value':'refreshAttempts'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unpivoted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Convert JSON column to DataFrame\n",
    "df_refreshAttempts = pd.json_normalize(df_unpivoted['refreshAttempts'])\n",
    "\n",
    "# Merge the original DataFrame with the new DataFrame\n",
    "df_merged = pd.concat([df_unpivoted.drop('refreshAttempts', axis=1), df_refreshAttempts], axis=1)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_clean = df_merged[['requestId','attemptId','startTime','endTime','serviceExceptionJson']]\n",
    "refresh_history_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_cleanest = df2.merge(refresh_history_clean, on='requestId', how='left')\n",
    "refresh_history_cleanest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_cleanest.drop(columns=['refreshAttempts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refresh_history_cleanest.to_csv('refresh_history_cleanest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['--base','true','--fullscan','30']\n",
    "for i in range(0, len(args), 2):\n",
    "    print(args[i])\n",
    "    # if args[i] == '--base':\n",
    "    #     print(args[i+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.fabric.microsoft.com/v1/admin/domains\", headers=headers)\n",
    "if response.ok:\n",
    "    print(response.json())\n",
    "    v = response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = v.get(\"domains\")\n",
    "for domain in domains:\n",
    "    domainId = domain['id']\n",
    "\n",
    "    response = requests.get(f\"https://api.fabric.microsoft.com/v1/admin/domains/{domainId}/workspaces\", headers=headers)\n",
    "    if response.ok:\n",
    "        print(response.json())\n",
    "    else:\n",
    "        print(response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.powerbi.com/v1.0/myorg/admin/capacities\"\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.ok:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.fabric.microsoft.com/v1/admin/workspaces\", headers=headers)\n",
    "if response.ok:\n",
    "    results = response.json()\n",
    "\n",
    "workspace = results.get(\"workspaces\")\n",
    "\n",
    "items = list()\n",
    "\n",
    "for item in workspace:\n",
    "    items.append(item[\"id\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "url = f\"https://api.powerbi.com/v1.0/myorg/admin/groups/ef61e238-9402-4a64-9107-1e10f5d0fcc4?$expand=users\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "if response.ok:\n",
    "    results = response.json()\n",
    "    print(results)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items = set(items)\n",
    "workspace_lst = list()\n",
    "\n",
    "ceiling = len(items)\n",
    "print(ceiling)\n",
    "cnt = 0\n",
    "for item in items:\n",
    "    cnt+=1\n",
    "\n",
    "    if len(workspace_lst) > ceiling:\n",
    "        break\n",
    "\n",
    "    if cnt <= ceiling:\n",
    "        url = f\"https://api.powerbi.com/v1.0/myorg/admin/groups/{item}?$expand=users\"\n",
    "    \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.ok:\n",
    "            results = response.json()\n",
    "            workspace_lst.append(results)\n",
    "        else:\n",
    "            if response.status_code==429:\n",
    "                result = response.json()\n",
    "                print(f\"you must wait { int(result.get('message').split('.')[1].split(' ')[3])/60} minutes\")\n",
    "\n",
    "                break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(workspace_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_json(json.dumps(workspace_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\"message\": \"You have exceeded the amount of requests allowed in the current time frame and further requests will fail. Retry in 1223 seconds.\"}\n",
    "message = response.get(\"message\")\n",
    "\n",
    "tm = message.split(sep=\".\")[1].split(sep=\" \")[3]\n",
    "\n",
    "print(tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Sample list of 50000 workspace IDs\n",
    "workspace_ids = list(range(1, 50001))\n",
    "\n",
    "# Function to process a single group of workspace IDs\n",
    "def process_group(group):\n",
    "    # Placeholder function to process a group\n",
    "    print(f\"Processing group with {len(group)} workspace IDs\")\n",
    "\n",
    "# Split the list into groups of 100 workspace IDs\n",
    "groups = [workspace_ids[i:i + 100] for i in range(0, len(workspace_ids), 100)]\n",
    "\n",
    "# Function to process groups in batches of 16\n",
    "def process_batches(groups):\n",
    "    batch_size = 16\n",
    "    while groups:\n",
    "        # Take the first 16 groups (or fewer if less than 16 remain)\n",
    "        batch = groups[:batch_size]\n",
    "        # Process the batch in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            executor.map(process_group, batch)\n",
    "        # Remove the processed groups from the list\n",
    "        groups = groups[batch_size:]\n",
    "        # Check if the number of remaining groups is less than 16\n",
    "        if len(groups) < batch_size:\n",
    "            print(\"Adding next group to be tested\")\n",
    "\n",
    "# Process the groups in batches\n",
    "process_batches(groups)\n",
    "\n",
    "# This script does the following:\n",
    "# 1. Creates a sample list of 50000 workspace IDs.\n",
    "# 2. Defines a `process_group` function to process a single group of workspace IDs.\n",
    "# 3. Splits the list into groups of 100 workspace IDs.\n",
    "# 4. Defines a `process_batches` function to process the groups in batches of 16, running them in parallel using `ThreadPoolExecutor` and removing each group from the batch once processed.\n",
    "\n",
    "# Feel free to modify the `process_group` function to suit your specific needs. Let me know if you have any questions or need further assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got it! Here's a Python script that will help you achieve this. The script will chunk the list of 5000 workspace IDs into groups of 100, process them in parallel with a maximum of 16 concurrent functions, and add new functions as soon as any of the 16 completes.\n",
    "\n",
    "# ```python\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "# Sample list of 5000 workspace IDs\n",
    "workspace_ids = list(range(1, 50001))\n",
    "\n",
    "# Function to process a single group of workspace IDs\n",
    "def process_group(group):\n",
    "    # Placeholder function to process a group\n",
    "    print(f\"Processing group with {len(group)} workspace IDs\")\n",
    "    for workspace_id in group:\n",
    "        # Placeholder processing for each workspace ID\n",
    "        print(f\"Processing workspace ID: {workspace_id}\")\n",
    "    print(\"Sleeping for 5 seconds\")\n",
    "    time.sleep(5)  # Simulate some processing time\n",
    "\n",
    "# Split the list into groups of 100 workspace IDs\n",
    "groups = [workspace_ids[i:i + 100] for i in range(0, len(workspace_ids), 100)]\n",
    "\n",
    "# Function to process groups in parallel with a maximum of 16 concurrent functions\n",
    "def process_batches(groups):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        futures = {executor.submit(process_group, group): group for group in groups[:16]}\n",
    "        remaining_groups = groups[16:]\n",
    "\n",
    "        while futures:\n",
    "            # Wait for the next function to complete\n",
    "            done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "\n",
    "            for future in done:\n",
    "                futures.pop(future)\n",
    "                if remaining_groups:\n",
    "                    next_group = remaining_groups.pop(0)\n",
    "                    futures[executor.submit(process_group, next_group)] = next_group\n",
    "\n",
    "# Process the groups in batches\n",
    "process_batches(groups)\n",
    "# ```\n",
    "\n",
    "# This script does the following:\n",
    "# 1. Creates a sample list of 5000 workspace IDs.\n",
    "# 2. Defines a `process_group` function to process a single group of workspace IDs.\n",
    "# 3. Splits the list into groups of 100 workspace IDs.\n",
    "# 4. Defines a `process_batches` function to process the groups in parallel with a maximum of 16 concurrent functions using `ThreadPoolExecutor`. It adds new functions as soon as any of the 16 completes.\n",
    "\n",
    "# Feel free to modify the `process_group` function to suit your specific needs. Let me know if you have any questions or need further assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Sample list of 5000 workspace IDs\n",
    "workspace_ids = list(range(1, 5001))\n",
    "\n",
    "# Async function to process a single group of workspace IDs\n",
    "async def process_group(group):\n",
    "    # Placeholder function to process a group\n",
    "    print(f\"Processing group with {len(group)} workspace IDs\")\n",
    "    await asyncio.sleep(1)  # Simulate some processing time\n",
    "\n",
    "# Split the list into groups of 100 workspace IDs\n",
    "groups = [workspace_ids[i:i + 100] for i in range(0, len(workspace_ids), 100)]\n",
    "\n",
    "# Function to process groups in parallel with a maximum of 16 concurrent functions\n",
    "async def process_batches(groups):\n",
    "    semaphore = asyncio.Semaphore(16)\n",
    "\n",
    "    async def process_with_semaphore(group):\n",
    "        async with semaphore:\n",
    "            await process_group(group)\n",
    "\n",
    "    tasks = [process_with_semaphore(group) for group in groups]\n",
    "    await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To achieve this, you can use Python's `asyncio` and `concurrent.futures` modules to chunk your list of 50,000 workspace IDs into groups of 100 and process them using an async function with a maximum of 16 concurrent threads. Here's a step-by-step guide to help you:\n",
    "\n",
    "# 1. **Chunk the list into groups of 100**: You can use a simple function to split your list into smaller chunks.\n",
    "# 2. **Create an async function to process each chunk**: This function will handle the processing of each group of workspace IDs.\n",
    "# 3. **Use a semaphore to limit the number of concurrent async tasks**: This will ensure that no more than 16 async tasks are running at the same time.\n",
    "# 4. **Run the async tasks using `asyncio`**: Use the event loop to manage the execution of the async tasks.\n",
    "\n",
    "# Here's an example to illustrate this:\n",
    "\n",
    "# ```python\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to chunk the list into groups of 100\n",
    "def chunk_list(lst, chunk_size):\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "# Async function to process each chunk\n",
    "async def process_chunk(chunk):\n",
    "    await asyncio.sleep(1)  # Simulate async processing\n",
    "    return f\"Processed {len(chunk)} workspace IDs\"\n",
    "\n",
    "# Function to run async tasks with a semaphore\n",
    "async def run_async_tasks(workspace_ids, max_concurrent_tasks):\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_tasks)\n",
    "    \n",
    "    async def sem_task(chunk):\n",
    "        async with semaphore:\n",
    "            return await process_chunk(chunk)\n",
    "    \n",
    "    tasks = [sem_task(chunk) for chunk in chunk_list(workspace_ids, 100)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# Main function to execute the async tasks\n",
    "def main():\n",
    "    workspace_ids = list(range(50000))  # Example list of 50,000 workspace IDs\n",
    "    max_concurrent_tasks = 16\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    results = loop.run_until_complete(run_async_tasks(workspace_ids, max_concurrent_tasks))\n",
    "    loop.close()\n",
    "    \n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "\n",
    "main()\n",
    "# ```\n",
    "\n",
    "# In this example:\n",
    "# - The `chunk_list` function splits the list of workspace IDs into chunks of 100.\n",
    "# - The `process_chunk` async function simulates processing each chunk.\n",
    "# - The `run_async_tasks` function uses a semaphore to limit the number of concurrent async tasks to 16.\n",
    "# - The `main` function sets up the event loop and runs the async tasks.\n",
    "\n",
    "# This approach ensures that your async function processes the workspace IDs in chunks of 100, with a maximum of 16 concurrent threads, without exceeding the limit. If you have any specific requirements or encounter any issues, feel free to let me know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"name\":\"Brandon\",\"age\":30}\n",
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.powerbi.com/v1.0/myorg/admin/capacities/refreshables\"\n",
    "response = requests.get(url, headers=headers, json=json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"name\" in d:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "group_size = 100\n",
    "input_list = list(range(1, 50001))\n",
    "\n",
    "groups = [input_list[i:i + group_size] for i in range(0, len(input_list), group_size)]\n",
    "\n",
    "def process_items(items):\n",
    "    counter = int()\n",
    "    for i in range(0, len(items), 16):\n",
    "        batch = items[i:i+16]\n",
    "\n",
    "        for j in batch:\n",
    "            counter+=1\n",
    "            print(f\"What row am I on {counter} and Processing item: {j}\")\n",
    "        # Process the batch\n",
    "        # print(f\"Processing batch: {batch} and length is {len(batch)}\")\n",
    "        time.sleep(2)  # Simulate processing time for each batch\n",
    "\n",
    "        if i % 16 == 0:\n",
    "            print(\"Pausing for 10 minutes...\")\n",
    "            time.sleep(2)  # Pause for 10 minutes\n",
    "\n",
    "# Example usage\n",
    "\n",
    "process_items(groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(groups[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((532/16) * 30)/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
