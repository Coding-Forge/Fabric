{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from env.audit import Audits\n",
    "\n",
    "appsecret = os.getenv(\"admin_appsecret\")\n",
    "appid = os.getenv(\"admin_appid\")\n",
    "tenantid = os.getenv(\"admin_tenantid\")\n",
    "\n",
    "audit = Audits()\n",
    "audit.set_ServicePrincipal(\n",
    "    tenant_id=tenantid,\n",
    "    client_id=appid,\n",
    "    client_secret= appsecret\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# audit.set_ApplicationModules(\"Apps\")\n",
    "audit.set_ApplicationModules(\"Activity,Apps,Capacity,Catalog,Domains,FabricItems,Gateway,Graph,Refreshables,RefreshHistory,Roles,Tenant,Workspaces\")\n",
    "# audit.set_ApplicationModules(\"Activity,Apps\")\n",
    "# audit.set_ApplicationModules(\"Roles,Tenant,Workspaces\")\n",
    "\n",
    "# a list of the supported logging levels\n",
    "import logging\n",
    "Logging_levels = {\n",
    "            \"DEBUG\": logging.DEBUG,\n",
    "            \"INFO\": logging.INFO,\n",
    "            \"WARNING\": logging.WARNING,\n",
    "            \"ERROR\": logging.ERROR,\n",
    "            \"CRITICAL\": logging.CRITICAL\n",
    "}\n",
    "\n",
    "audit.set_LakehouseName(\"FabricLake\")\n",
    "audit.set_PathInLakehouse(\"stage\")\n",
    "audit.set_WorkspaceName(\"FabricMonitor\")\n",
    "audit.set_logging_config(\"INFO\", \"brandon.log\")\n",
    "\n",
    "audit.set_Activity_cron(\"* * * * * 30\")\n",
    "audit.set_Catalog_cron(\"* * * * * 30\")\n",
    "audit.set_Capacity_cron(\"* * * * * 30\")\n",
    "audit.set_Apps_cron(\"* * * * * 30\")\n",
    "audit.set_Domains_cron(\"* * * * * 30\") # Not yet implemented\n",
    "audit.set_Graph_cron(\"* * * * * 30\")\n",
    "audit.set_Tenant_cron(\"* * * * * 30\")\n",
    "audit.set_RefreshHistory_cron(\"* * * * * 30\")\n",
    "audit.set_Refreshables_cron(\"* * * * * 30\")\n",
    "audit.set_Gateway_cron(\"* * * * * 30\")\n",
    "\n",
    "# print(audit.ServicePrincipal)\n",
    "await audit.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.set(\"fs.azure.account.auth.type.onelake.dfs.core.windows.net\", \"OAuth\")\n",
    "sparkConf.set(\"fs.azure.account.oauth.provider.type.onelake.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.id.onelake.dfs.core.windows.net\", appid)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.secret.onelake.dfs.core.windows.net\", appsecret)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.endpoint.onelake.dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantid)\n",
    "sparkConf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1\")\n",
    "spark = SparkSession.builder.config(conf=sparkConf).appName(\"SparkTest\").getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "data = [(\"John\", \"30\", now), (\"Jane\", \"25\", now)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"source\", lit(\"test\"))\n",
    "\n",
    "df.write.format(\"parquet\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n",
    "# df.write.format(\"parquet\").mode(\"append\").save(\"FabricLake.Lakehouse/Files/data.parquet\")\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data\")\n",
    "df.write.format(\"parquet\").save(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a DataLakeServiceClient using a connection string\n",
    "   from azure.storage.filedatalake import DataLakeServiceClient\n",
    "   datalake_service_client = DataLakeServiceClient.from_connection_string(self.connection_string)\n",
    "\n",
    "   # Instantiate a FileSystemClient\n",
    "   file_system_client = datalake_service_client.get_file_system_client(\"mynewfilesystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "storage_account_name = \"onelake\"\n",
    "workspace_name = \"FabricMonitor\"\n",
    "\n",
    "credential = ClientSecretCredential(tenant_id=tenantid,\n",
    "                            client_id=appid,\n",
    "                            client_secret=appsecret)\n",
    "\n",
    "file_system_client = FileSystemClient(\n",
    "    account_url=\"https://onelake.dfs.fabric.microsoft.com\",\n",
    "    file_system_name=workspace_name,\n",
    "    credential=credential)\n",
    "\n",
    "dc = file_system_client.get_directory_client(\"FabricLake.Lakehouse/Files\")\n",
    "file_client = dc.get_file_client(\"data.parquet\")\n",
    "with open(\"./data.parquet\", \"rb\") as data:\n",
    "    file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
