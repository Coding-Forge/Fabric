{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from env.audit import Audits\n",
    "\n",
    "appsecret = os.getenv(\"admin_appsecret\")\n",
    "appid = os.getenv(\"admin_appid\")\n",
    "tenantid = os.getenv(\"admin_tenantid\")\n",
    "\n",
    "audit = Audits()\n",
    "audit.set_ServicePrincipal(\n",
    "    tenant_id=tenantid,\n",
    "    client_id=appid,\n",
    "    client_secret= appsecret\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the current state {\"activity\": {\"lastRun\": \"2024-10-22T15:04:02.092292Z\"}, \"apps\": {\"lastRun\": \"2024-10-22T15:04:02.092318Z\"}, \"catalog\": {\"lastRun\": \"2024-10-22T12:37:01.659785Z\"}, \"gateway\": {\"lastRun\": \"2024-10-22T15:04:02.092337Z\"}, \"graph\": {\"lastRun\": \"2024-10-22T15:04:02.092340Z\"}, \"refreshables\": {\"lastRun\": \"2024-10-22T15:04:02.092344Z\"}, \"refreshhistory\": {\"lastRun\": \"2024-10-22T15:04:02.092348Z\"}, \"tenant\": {\"lastRun\": \"2024-10-22T15:04:02.092351Z\"}, \"capacity\": {\"lastRun\": \"2024-10-22T15:04:02.092322Z\"}, \"roles\": {\"lastRun\": \"2024-09-23T14:28:41.625610Z\"}, \"domains\": {\"lastRun\": \"2024-10-22T11:18:56.147146Z\"}, \"fabricitems\": {\"lastRun\": \"2024-10-22T11:18:56.147193Z\"}, \"workspaces\": {\"lastRun\": \"2024-10-22T11:18:56.147202Z\"}}\n",
      "Task Catalog Scans (Catalog meta data) is now running\n",
      "using the following url: admin/workspaces/modified?excludePersonalWorkspaces=True&excludeInActiveWorkspaces=True\n",
      "Task Catalog Scans (Catalog meta data) is now running\n",
      "using the following url: admin/workspaces/modified?excludePersonalWorkspaces=True&excludeInActiveWorkspaces=True\n",
      "Processing batch number 1 of 0 batches\n",
      "Processing item number 1\n",
      "Processing batch number 1 of 0 batches\n",
      "Processing item number 1\n",
      "Saving scan results to 20241022_00001.scanResults.fullscan.json\n",
      "Saved\n",
      "I sleeping\n",
      "Task Catalog elapsed time: 33.3\n",
      "Saving scan results to 20241022_00001.scanResults.fullscan.json\n",
      "Saved\n",
      "I sleeping\n",
      "Task Catalog elapsed time: 63.2\n",
      "\n",
      "Total elapsed time: 63.7\n"
     ]
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "\n",
    "# from notebookutils import mssparkutils\n",
    "from azure.identity import ClientSecretCredential, DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "LakehouseName = \"LochMonitor\"\n",
    "WarehouseName = \"\"\n",
    "WorkspaceName = \"FabricMonitor\"\n",
    "\n",
    "\n",
    "# must have\n",
    "audit.set_LakehouseName(LakehouseName)  \n",
    "audit.set_WorkspaceName(WorkspaceName) \n",
    "\n",
    "# optional\n",
    "# audit.set_ApplicationModules(\"Activity,Apps,Capacity,Catalog,Domains,FabricItems,Gateway,Graph,Refreshables,RefreshHistory,Tenant,Workspaces\")\n",
    "audit.set_ApplicationModules(\"Catalog\")\n",
    "audit.set_PathInLakehouse(\"stage\")\n",
    "audit.set_all_workspaces(True)\n",
    "\n",
    "audit.exclude_personal_workspaces = True\n",
    "audit.exclude_inactivate_workspaces = True\n",
    "\n",
    "\n",
    "# set cron for modules \n",
    "# audit.set_Activity_cron(\"* * * * * 30\")\n",
    "# audit.set_Catalog_cron(\"* * * * * 30\")\n",
    "# audit.set_Capacity_cron(\"* * * * * 30\")\n",
    "# audit.set_Apps_cron(\"* * * * * 30\")\n",
    "# audit.set_Domains_cron(\"* * * * * 30\")\n",
    "# audit.set_Graph_cron(\"* * * * * 30\")\n",
    "# audit.set_Tenant_cron(\"* * * * * 30\")\n",
    "# audit.set_RefreshHistory_cron(\"* * * * * 30\")\n",
    "# audit.set_Refreshables_cron(\"* * * * * 30\")\n",
    "# audit.set_Gateway_cron(\"* * * * * 30\")\n",
    "\n",
    "# Run the application\n",
    "await audit.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSERT Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Assuming you have a Delta table named \"my_delta_table\"\n",
    "delta_table_path = \"path/to/delta-table\"  # Replace with your actual path\n",
    "\n",
    "# Create or load your DataFrame (myDataFrame) with the data to be upserted\n",
    "# For demonstration purposes, let's assume you have columns: \"Id\", \"Type\", \"Value\"\n",
    "\n",
    "# Initialize the DeltaTable\n",
    "# delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "myDataFrame = spark.read.format(\"delta\").load(myDataFrame_path)\n",
    "\n",
    "# Merge the updates into the Delta table\n",
    "delta_table.alias(\"events\") \\\n",
    "    .merge(myDataFrame.alias(\"updates\"), \"events.Id = updates.Id and events.Type = updates.Type\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Commit the changes\n",
    "delta_table.toDF().write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Assuming delta_table and myDataFrame are already defined\n",
    "# delta_table_path = \"path/to/delta-table\"  # Replace with your actual path\n",
    "\n",
    "# Initialize the DeltaTable\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Merge the updates into the Delta table using datasetId for comparison\n",
    "delta_table.alias(\"existing\") \\\n",
    "    .merge(\n",
    "        myDataFrame.alias(\"updates\"),\n",
    "        \"existing.datasetId = updates.datasetId\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Commit the changes\n",
    "delta_table.toDF().write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields, InitVar\n",
    "\n",
    "@dataclass\n",
    "class Rectangle:\n",
    "    width: int\n",
    "    height: int\n",
    "\n",
    "    def area(self):\n",
    "        return self.width * self.height\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * (self.width + self.height)\n",
    "    \n",
    "@dataclass\n",
    "class ColoredRectangle(Rectangle):\n",
    "    color: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class C:\n",
    "    i: int\n",
    "    j: int | None = None\n",
    "    database: InitVar[str|None] = None\n",
    "\n",
    "    def __post_init__(self, database):\n",
    "        if self.j is None and database is not None:\n",
    "            self.j = database.lookup(\"j\")\n",
    "\n",
    "\n",
    "c = C(10, database={\"j\": \"value\"})\n",
    "print(c.j)\n",
    "rect = ColoredRectangle(3, 4, 'blue')\n",
    "\n",
    "print(rect.area())\n",
    "print(rect.color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_key_nested(d, key):\n",
    "    if key in d:\n",
    "        return True\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            if is_key_nested(v, key):\n",
    "                return True\n",
    "        elif isinstance(v, list):\n",
    "            for item in v:\n",
    "                if isinstance(item, dict) and is_key_nested(item, key):\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "is_datasets_nested = is_key_nested(myDict, 'datasets')\n",
    "print(is_datasets_nested)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import SearchIndex, SimpleField, SearchFieldDataType\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Read the binary file\n",
    "file_path = \"path/to/your/binary/file\"\n",
    "with open(file_path, \"rb\") as file:\n",
    "    binary_content = file.read()\n",
    "\n",
    "# Convert binary content to text (assuming it's text-based binary like PDF)\n",
    "text_content = binary_content.decode('utf-8')\n",
    "\n",
    "# Split the text content into chunks\n",
    "chunks = text_splitter.split_text(text_content)\n",
    "\n",
    "# Azure Search service details\n",
    "search_service_name = \"your-search-service-name\"\n",
    "index_name = \"your-index-name\"\n",
    "admin_key = \"your-admin-key\"\n",
    "\n",
    "# Create a SearchClient\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{search_service_name}.search.windows.net\",\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(admin_key)\n",
    ")\n",
    "\n",
    "# Insert chunks into Azure AI Search Index\n",
    "for i, chunk in enumerate(chunks):\n",
    "    document = {\n",
    "        \"id\": str(i),\n",
    "        \"content\": chunk\n",
    "    }\n",
    "    search_client.upload_documents(documents=[document])\n",
    "\n",
    "print(\"Chunks inserted into Azure AI Search Index successfully.\")\n",
    "# Initialize the OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embedded_chunks = [embeddings.embed(chunk) for chunk in chunks]\n",
    "\n",
    "# Insert embedded chunks into Azure AI Search Index\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embedded_chunks)):\n",
    "    document = {\n",
    "        \"id\": str(i),\n",
    "        \"content\": chunk,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "    search_client.upload_documents(documents=[document])\n",
    "\n",
    "print(\"Chunks with embeddings inserted into Azure AI Search Index successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in myDict:\n",
    "    print(myDict[key])\n",
    "    if isinstance(myDict[key], dict):\n",
    "        for k in myDict[key]:\n",
    "            print(k)\n",
    "    if isinstance(myDict[key], list):\n",
    "        for i in range(0, len(myDict[key])):\n",
    "            for j in myDict[key][0]:\n",
    "                print(myDict[key][0][j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.set(\"fs.azure.account.auth.type.onelake.dfs.core.windows.net\", \"OAuth\")\n",
    "sparkConf.set(\"fs.azure.account.oauth.provider.type.onelake.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.id.onelake.dfs.core.windows.net\", appid)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.secret.onelake.dfs.core.windows.net\", appsecret)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.endpoint.onelake.dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantid)\n",
    "sparkConf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1\")\n",
    "spark = SparkSession.builder.config(conf=sparkConf).appName(\"SparkTest\").getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "data = [(\"John\", \"30\", now), (\"Jane\", \"25\", now)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"source\", lit(\"test\"))\n",
    "\n",
    "df.write.format(\"parquet\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n",
    "# df.write.format(\"parquet\").mode(\"append\").save(\"FabricLake.Lakehouse/Files/data.parquet\")\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data\")\n",
    "df.write.format(\"parquet\").save(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a DataLakeServiceClient using a connection string\n",
    "   from azure.storage.filedatalake import DataLakeServiceClient\n",
    "   datalake_service_client = DataLakeServiceClient.from_connection_string(self.connection_string)\n",
    "\n",
    "   # Instantiate a FileSystemClient\n",
    "   file_system_client = datalake_service_client.get_file_system_client(\"mynewfilesystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "\n",
    "counter = int()\n",
    "\n",
    "for i in range(1,84):\n",
    "    counter+=1\n",
    "\n",
    "    if i % 17 == 0:\n",
    "        print(datetime.datetime.now())\n",
    "        sleep(5)\n",
    "        print(f\"FizzBuzz and {counter}\")\n",
    "        # print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = range(1,15000)\n",
    "\n",
    "def distribute_items(items, num_groups):\n",
    "    # Calculate the size of each group\n",
    "    group_size = len(items) // num_groups\n",
    "    remainder = len(items) % num_groups\n",
    "\n",
    "    # Create the groups\n",
    "    groups = []\n",
    "    start = 0\n",
    "    for i in range(num_groups):\n",
    "        end = start + group_size + (1 if i < remainder else 0)\n",
    "        groups.append(items[start:end])\n",
    "        start = end\n",
    "\n",
    "    return groups\n",
    "\n",
    "# Example usage\n",
    "items = list(range(1, 15001))  # List of 15,000 items\n",
    "num_groups = 16\n",
    "groups = distribute_items(items, num_groups)\n",
    "\n",
    "# Print the size of each group\n",
    "for i, group in enumerate(groups):\n",
    "    print(f\"Group {i+1}: {len(group)} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "storage_account_name = \"onelake\"\n",
    "workspace_name = \"FabricMonitor\"\n",
    "\n",
    "credential = ClientSecretCredential(tenant_id=tenantid,\n",
    "                            client_id=appid,\n",
    "                            client_secret=appsecret)\n",
    "\n",
    "file_system_client = FileSystemClient(\n",
    "    account_url=\"https://onelake.dfs.fabric.microsoft.com\",\n",
    "    file_system_name=workspace_name,\n",
    "    credential=credential)\n",
    "\n",
    "dc = file_system_client.get_directory_client(\"FabricLake.Lakehouse/Files\")\n",
    "file_client = dc.get_file_client(\"data.parquet\")\n",
    "with open(\"./data.parquet\", \"rb\") as data:\n",
    "    file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load data into pandas DataFrame from \"/lakehouse/default/\" + \"Files/stage/state.json\"\n",
    "state = pd.read_json(\"/lakehouse/default/\" + \"Files/stage/state.yaml\",typ=\"series\")\n",
    "\n",
    "lastRun = state['activity']['lastRun']\n",
    "lastRun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/lakehouse/default/\" + \"Files/stage/state.yaml\", \"r\") as file:\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spark.read.option(\"multiline\", \"true\").json(\"Files/stage/state.json\")\n",
    "df = spark.read.option(\"multiLines\",\"json\").json(\"Files/stage/catalog/scans/2024/07/22/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "\n",
    "data = df.withColumn(\"datasets\", explode(df[\"workspaces\"][\"datasets\"]))\n",
    "\n",
    "df = df.withColumn(\"start_Time\", to_utc_timestamp(df['startTime'], \"UTC\"))\n",
    "df = df.withColumn(\"cst_timestamp\", from_utc_timestamp(df.start_Time, \"America/Chicago\"))\n",
    "df = df.withColumn(\"eu_timestamp\", from_utc_timestamp(df.start_Time, \"Europe/Berlin\"))\n",
    "df = df.withColumn(\"ist_timestamp\", from_utc_timestamp(df.start_Time, \"Asia/Kolkata\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "\n",
    "dc = df.withColumn(\"requestId\", df.requestId)\\\n",
    "    .withColumn(\"refreshAttempts\", explode(df.refreshAttempts))\\\n",
    "    .select(\"requestId\",\"refreshAttempts\")\n",
    "\n",
    "display(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = dc.withColumn(\"attemptId\", dc.refreshAttempts.attemptId)\\\n",
    "    .withColumn(\"startTime\", dc.refreshAttempts.startTime)\\\n",
    "    .withColumn(\"endTime\", dc.refreshAttempts.endTime)\\\n",
    "    .withColumn(\"type\", dc.refreshAttempts.type)\\\n",
    "    .select(\"requestId\",\"attemptId\",\"startTime\",\"endTime\",\"type\")\n",
    "\n",
    "display(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('dataset_id', 'dataset_name', 'endTime', 'id', 'refreshType', 'requestId', 'serviceExceptionJson', 'startTime', 'status').dropDuplicates([\"dataset_id\",\"requestId\"]).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/refreshhistory\")\n",
    "de.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/refreshhistory_attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT dataset_id, COUNT(DISTINCT requestId) as requestId_count\n",
    "FROM refreshhistory\n",
    "GROUP BY dataset_id\n",
    "HAVING COUNT(DISTINCT requestId) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the latest record by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"Tables/workspaces_workspaces\"\n",
    "df = spark.read.format(\"delta\").load(ds)\n",
    "dd = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = dd.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(\"Files/stage/catalog/scans/2024/*/*/*.json\")\n",
    "dd = df.withColumn(\"workspaces\", explode(df.workspaces)).drop(\"datasourceInstances\")\n",
    "\n",
    "workspaces = dd.withColumn(\"capacityId\", dd[\"workspaces\"][\"capacityId\"]) \\\n",
    "    .withColumn(\"dataRetrievalState\", dd[\"workspaces\"][\"dataRetrievalState\"]) \\\n",
    "    .withColumn(\"defaultDatasetStorageFormat\", dd[\"workspaces\"][\"defaultDatasetStorageFormat\"]) \\\n",
    "    .withColumn(\"description\", dd[\"workspaces\"][\"description\"]) \\\n",
    "    .withColumn(\"domainId\", dd[\"workspaces\"][\"domainId\"]) \\\n",
    "    .withColumn(\"id\", dd[\"workspaces\"][\"id\"]) \\\n",
    "    .withColumn(\"isOnDedicatedCapacity\", dd[\"workspaces\"][\"isOnDedicatedCapacity\"]) \\\n",
    "    .withColumn(\"name\", dd[\"workspaces\"][\"name\"]) \\\n",
    "    .withColumn(\"state\", dd[\"workspaces\"][\"state\"]) \\\n",
    "    .withColumn(\"type\", dd[\"workspaces\"][\"type\"]) \n",
    "\n",
    "\n",
    "ds = dd.withColumn(\"id\", dd.workspaces.id)\\\n",
    "    .withColumn(\"datasets\", explode(dd.workspaces.datasets)).drop(\"workspaces\")\n",
    "\n",
    "ds = ds.withColumn(\"dataset_id\", ds[\"datasets\"][\"id\"])\n",
    "\n",
    "ds.select(\"id\",\"dataset_id\").dropDuplicates().write.format(\"delta\").mode(\"overwrite\").save(\"Tables/catalog_workspace_datasets\")\n",
    "\n",
    "ee = spark.read.format(\"delta\").load(\"Tables/datasetrefreshable_datasetrefreshable\")\n",
    "display(ee)\n",
    "\n",
    "workspaces = dd.withColumn(\"id\", dd[\"workspaces\"][\"id\"]) \\\n",
    "            .withColumn(\"defaultDatasetStorageFormat\",dd[\"workspaces\"][\"defaultDatasetStorageFormat\"])\\\n",
    "            .withColumn(\"description\",dd[\"workspaces\"][\"description\"])\\\n",
    "            .withColumn(\"domainId\",dd[\"workspaces\"][\"domainId\"])\\\n",
    "            .withColumn(\"isOnDedicatedCapacity\",df[\"workspaces\"][\"isOnDedicatedCapacity\"])\\\n",
    "            .withColumn(\"datasets\",explode(df[\"workspaces\"][\"datasets\"]))\n",
    "\n",
    "dd = df.withColumn(\"workspaces\", explode(df.workspaces)).drop(\"datasourceInstances\")\n",
    "display(dd)\n",
    "\n",
    "display(dd.select(dd[\"workspaces\"][\"id\"]))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets\")\n",
    "display(df.limit(10))\n",
    "\n",
    "datasets = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "display(datasets.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"Tables/workspaces_workspaces\"\n",
    "df = spark.read.format(\"delta\").load(ds)\n",
    "dd = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = dd.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(ds)\n",
    "\n",
    "latest_records_df.write.format(\"delta\").save(\"Tables/catalog_datasets_unique\")\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_tables\")\n",
    "df_tables = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_columns\")\n",
    "df_columns = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_measures\")\n",
    "df_measures = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = df_measures.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "tables = df_tables.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "columns = df_columns.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "\n",
    "measures.write.format(\"delta\").save(\"Tables/catalog_datasets_measures_history\")\n",
    "tables.dropDuplicates(\"UID\").write.format(\"delta\").save(\"Tables/catalog_datasets_tables_history\")\n",
    "columns.write.format(\"delta\").save(\"Tables/catalog_datasets_columns_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_columns_history\")\n",
    "display(dd.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_tables_history\")\n",
    "cols = [\"datasets_id\",\"datasets_tables_name\",\"scandate\"]\n",
    "display(tables.orderBy(*cols).select(*cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.dropDuplicates([\"UID\"]).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"True\").save(\"Tables/catalog_datasets_tables_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select T.UID, datasets_tables_name, datasets_id, scandate, description, name FROM catalog_tables_history as T\n",
    "INNER JOIN (select UID, COUNT(UID) FROM catalog_tables_history group by UID having count(UID)>1) as G\n",
    "ON T.UID=G.UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"datasets_id\", \"datasets_tables_name\"]\n",
    "\n",
    "window_spec = Window.partitionBy(*cols).orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = df_tables.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.select(col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
