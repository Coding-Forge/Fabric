{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from env.audit import Audits\n",
    "\n",
    "appsecret = os.getenv(\"admin_appsecret\")\n",
    "appid = os.getenv(\"admin_appid\")\n",
    "tenantid = os.getenv(\"admin_tenantid\")\n",
    "\n",
    "audit = Audits()\n",
    "audit.set_ServicePrincipal(\n",
    "    tenant_id=tenantid,\n",
    "    client_id=appid,\n",
    "    client_secret= appsecret\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-16 11:06:13,694 - azure.identity._internal.get_token_mixin - WARNING - ClientSecretCredential.get_token failed: Authentication failed: AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'e15fadf9-bc05-4ade-af9f-d79a918bbacd'. Trace ID: 3b7a12a8-c5c6-48c7-89a7-ef6d82bc3f00 Correlation ID: d821e10c-559d-4fd8-aab3-1b8fb31d3cb2 Timestamp: 2024-09-16 15:06:13Z\n",
      "2024-09-16 11:06:32,863 - azure.identity._internal.get_token_mixin - WARNING - ClientSecretCredential.get_token failed: Authentication failed: AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'e15fadf9-bc05-4ade-af9f-d79a918bbacd'. Trace ID: ab675d1f-98ce-4bdf-8bde-88267c563b00 Correlation ID: 03c197c2-82cd-45a0-bc0a-c872c85d90f1 Timestamp: 2024-09-16 15:06:32Z\n",
      "2024-09-16 11:06:56,255 - azure.identity._internal.get_token_mixin - WARNING - ClientSecretCredential.get_token failed: Authentication failed: AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'e15fadf9-bc05-4ade-af9f-d79a918bbacd'. Trace ID: c3010f75-294a-46f6-b6d2-6cf1ca086000 Correlation ID: 819b6625-c7ab-4509-b031-b4d7a9d2f53d Timestamp: 2024-09-16 15:06:56Z\n",
      "2024-09-16 11:07:39,238 - azure.identity._internal.get_token_mixin - WARNING - ClientSecretCredential.get_token failed: Authentication failed: AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'e15fadf9-bc05-4ade-af9f-d79a918bbacd'. Trace ID: 0748a84f-f612-404a-b0c3-fc8a0b386800 Correlation ID: 407a6f62-c375-4539-a3ab-a2ebd5a33852 Timestamp: 2024-09-16 15:07:39Z\n",
      "File not found: Authentication failed: AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'e15fadf9-bc05-4ade-af9f-d79a918bbacd'. Trace ID: 0748a84f-f612-404a-b0c3-fc8a0b386800 Correlation ID: 407a6f62-c375-4539-a3ab-a2ebd5a33852 Timestamp: 2024-09-16 15:07:39Z\n",
      "2024-09-16 11:07:39,241 - env.context - ERROR - Error reading file\n",
      "Error: 'NoneType' object has no attribute 'decode'\n",
      "what is the current state None\n",
      "Creating state.yaml file\n",
      "2024-09-16 11:07:39,843 - azure.identity._internal.get_token_mixin - WARNING - ClientSecretCredential.get_token failed: Authentication failed: AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'e15fadf9-bc05-4ade-af9f-d79a918bbacd'. Trace ID: c6da972a-86c9-463a-949c-e23fdd4c3500 Correlation ID: 3810d7a1-fe68-4b8f-8b35-c89c4969cdc9 Timestamp: 2024-09-16 15:07:39Z\n",
      "2024-09-16 11:07:56,532 - azure.identity._internal.get_token_mixin - WARNING - ClientSecretCredential.get_token failed: Authentication failed: AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'e15fadf9-bc05-4ade-af9f-d79a918bbacd'. Trace ID: 2c417f7c-845f-4104-91df-c8bc68ea4500 Correlation ID: 330d4f6c-ac7d-4753-9f79-b6c268cd7822 Timestamp: 2024-09-16 15:07:56Z\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# audit.set_ApplicationModules(\"Apps\")\n",
    "audit.set_ApplicationModules(\"Activity,Apps,Capacity,Catalog,Domains,FabricItems,Gateway,Graph,Refreshables,RefreshHistory,Roles,Tenant,Workspaces\")\n",
    "# audit.set_ApplicationModules(\"Activity,Apps\")\n",
    "# audit.set_ApplicationModules(\"Roles,Tenant,Workspaces\")\n",
    "\n",
    "# a list of the supported logging levels\n",
    "import logging\n",
    "Logging_levels = {\n",
    "            \"DEBUG\": logging.DEBUG,\n",
    "            \"INFO\": logging.INFO,\n",
    "            \"WARNING\": logging.WARNING,\n",
    "            \"ERROR\": logging.ERROR,\n",
    "            \"CRITICAL\": logging.CRITICAL\n",
    "}\n",
    "\n",
    "audit.set_LakehouseName(\"FabricLake\")\n",
    "audit.set_PathInLakehouse(\"stage\")\n",
    "audit.set_WorkspaceName(\"FabricMonitor\")\n",
    "audit.set_logging_config(\"INFO\", \"brandon.log\")\n",
    "audit.set_all_workspaces(True)\n",
    "\n",
    "audit.set_Activity_cron(\"* * * * * 30\")\n",
    "audit.set_Catalog_cron(\"* * * * * 30\")\n",
    "audit.set_Capacity_cron(\"* * * * * 30\")\n",
    "audit.set_Apps_cron(\"* * * * * 30\")\n",
    "audit.set_Domains_cron(\"* * * * * 30\") # Not yet implemented\n",
    "audit.set_Graph_cron(\"* * * * * 30\")\n",
    "audit.set_Tenant_cron(\"* * * * * 30\")\n",
    "audit.set_RefreshHistory_cron(\"* * * * * 30\")\n",
    "audit.set_Refreshables_cron(\"* * * * * 30\")\n",
    "audit.set_Gateway_cron(\"* * * * * 30\")\n",
    "\n",
    "# print(audit.ServicePrincipal)\n",
    "await audit.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSERT Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Assuming you have a Delta table named \"my_delta_table\"\n",
    "delta_table_path = \"path/to/delta-table\"  # Replace with your actual path\n",
    "\n",
    "# Create or load your DataFrame (myDataFrame) with the data to be upserted\n",
    "# For demonstration purposes, let's assume you have columns: \"Id\", \"Type\", \"Value\"\n",
    "\n",
    "# Initialize the DeltaTable\n",
    "# delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "myDataFrame = spark.read.format(\"delta\").load(myDataFrame_path)\n",
    "\n",
    "# Merge the updates into the Delta table\n",
    "delta_table.alias(\"events\") \\\n",
    "    .merge(myDataFrame.alias(\"updates\"), \"events.Id = updates.Id and events.Type = updates.Type\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Commit the changes\n",
    "delta_table.toDF().write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.set(\"fs.azure.account.auth.type.onelake.dfs.core.windows.net\", \"OAuth\")\n",
    "sparkConf.set(\"fs.azure.account.oauth.provider.type.onelake.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.id.onelake.dfs.core.windows.net\", appid)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.secret.onelake.dfs.core.windows.net\", appsecret)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.endpoint.onelake.dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantid)\n",
    "sparkConf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1\")\n",
    "spark = SparkSession.builder.config(conf=sparkConf).appName(\"SparkTest\").getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "data = [(\"John\", \"30\", now), (\"Jane\", \"25\", now)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"source\", lit(\"test\"))\n",
    "\n",
    "df.write.format(\"parquet\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n",
    "# df.write.format(\"parquet\").mode(\"append\").save(\"FabricLake.Lakehouse/Files/data.parquet\")\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data\")\n",
    "df.write.format(\"parquet\").save(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a DataLakeServiceClient using a connection string\n",
    "   from azure.storage.filedatalake import DataLakeServiceClient\n",
    "   datalake_service_client = DataLakeServiceClient.from_connection_string(self.connection_string)\n",
    "\n",
    "   # Instantiate a FileSystemClient\n",
    "   file_system_client = datalake_service_client.get_file_system_client(\"mynewfilesystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "storage_account_name = \"onelake\"\n",
    "workspace_name = \"FabricMonitor\"\n",
    "\n",
    "credential = ClientSecretCredential(tenant_id=tenantid,\n",
    "                            client_id=appid,\n",
    "                            client_secret=appsecret)\n",
    "\n",
    "file_system_client = FileSystemClient(\n",
    "    account_url=\"https://onelake.dfs.fabric.microsoft.com\",\n",
    "    file_system_name=workspace_name,\n",
    "    credential=credential)\n",
    "\n",
    "dc = file_system_client.get_directory_client(\"FabricLake.Lakehouse/Files\")\n",
    "file_client = dc.get_file_client(\"data.parquet\")\n",
    "with open(\"./data.parquet\", \"rb\") as data:\n",
    "    file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load data into pandas DataFrame from \"/lakehouse/default/\" + \"Files/stage/state.json\"\n",
    "state = pd.read_json(\"/lakehouse/default/\" + \"Files/stage/state.yaml\",typ=\"series\")\n",
    "\n",
    "lastRun = state['activity']['lastRun']\n",
    "lastRun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/lakehouse/default/\" + \"Files/stage/state.yaml\", \"r\") as file:\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spark.read.option(\"multiline\", \"true\").json(\"Files/stage/state.json\")\n",
    "df = spark.read.option(\"multiLines\",\"json\").json(\"Files/stage/catalog/scans/2024/07/22/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "\n",
    "data = df.withColumn(\"datasets\", explode(df[\"workspaces\"][\"datasets\"]))\n",
    "\n",
    "df = df.withColumn(\"start_Time\", to_utc_timestamp(df['startTime'], \"UTC\"))\n",
    "df = df.withColumn(\"cst_timestamp\", from_utc_timestamp(df.start_Time, \"America/Chicago\"))\n",
    "df = df.withColumn(\"eu_timestamp\", from_utc_timestamp(df.start_Time, \"Europe/Berlin\"))\n",
    "df = df.withColumn(\"ist_timestamp\", from_utc_timestamp(df.start_Time, \"Asia/Kolkata\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "\n",
    "dc = df.withColumn(\"requestId\", df.requestId)\\\n",
    "    .withColumn(\"refreshAttempts\", explode(df.refreshAttempts))\\\n",
    "    .select(\"requestId\",\"refreshAttempts\")\n",
    "\n",
    "display(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = dc.withColumn(\"attemptId\", dc.refreshAttempts.attemptId)\\\n",
    "    .withColumn(\"startTime\", dc.refreshAttempts.startTime)\\\n",
    "    .withColumn(\"endTime\", dc.refreshAttempts.endTime)\\\n",
    "    .withColumn(\"type\", dc.refreshAttempts.type)\\\n",
    "    .select(\"requestId\",\"attemptId\",\"startTime\",\"endTime\",\"type\")\n",
    "\n",
    "display(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('dataset_id', 'dataset_name', 'endTime', 'id', 'refreshType', 'requestId', 'serviceExceptionJson', 'startTime', 'status').dropDuplicates([\"dataset_id\",\"requestId\"]).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/refreshhistory\")\n",
    "de.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/refreshhistory_attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT dataset_id, COUNT(DISTINCT requestId) as requestId_count\n",
    "FROM refreshhistory\n",
    "GROUP BY dataset_id\n",
    "HAVING COUNT(DISTINCT requestId) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the latest record by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"Tables/workspaces_workspaces\"\n",
    "df = spark.read.format(\"delta\").load(ds)\n",
    "dd = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = dd.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(\"Files/stage/catalog/scans/2024/*/*/*.json\")\n",
    "dd = df.withColumn(\"workspaces\", explode(df.workspaces)).drop(\"datasourceInstances\")\n",
    "\n",
    "workspaces = dd.withColumn(\"capacityId\", dd[\"workspaces\"][\"capacityId\"]) \\\n",
    "    .withColumn(\"dataRetrievalState\", dd[\"workspaces\"][\"dataRetrievalState\"]) \\\n",
    "    .withColumn(\"defaultDatasetStorageFormat\", dd[\"workspaces\"][\"defaultDatasetStorageFormat\"]) \\\n",
    "    .withColumn(\"description\", dd[\"workspaces\"][\"description\"]) \\\n",
    "    .withColumn(\"domainId\", dd[\"workspaces\"][\"domainId\"]) \\\n",
    "    .withColumn(\"id\", dd[\"workspaces\"][\"id\"]) \\\n",
    "    .withColumn(\"isOnDedicatedCapacity\", dd[\"workspaces\"][\"isOnDedicatedCapacity\"]) \\\n",
    "    .withColumn(\"name\", dd[\"workspaces\"][\"name\"]) \\\n",
    "    .withColumn(\"state\", dd[\"workspaces\"][\"state\"]) \\\n",
    "    .withColumn(\"type\", dd[\"workspaces\"][\"type\"]) \n",
    "\n",
    "\n",
    "ds = dd.withColumn(\"id\", dd.workspaces.id)\\\n",
    "    .withColumn(\"datasets\", explode(dd.workspaces.datasets)).drop(\"workspaces\")\n",
    "\n",
    "ds = ds.withColumn(\"dataset_id\", ds[\"datasets\"][\"id\"])\n",
    "\n",
    "ds.select(\"id\",\"dataset_id\").dropDuplicates().write.format(\"delta\").mode(\"overwrite\").save(\"Tables/catalog_workspace_datasets\")\n",
    "\n",
    "ee = spark.read.format(\"delta\").load(\"Tables/datasetrefreshable_datasetrefreshable\")\n",
    "display(ee)\n",
    "\n",
    "workspaces = dd.withColumn(\"id\", dd[\"workspaces\"][\"id\"]) \\\n",
    "            .withColumn(\"defaultDatasetStorageFormat\",dd[\"workspaces\"][\"defaultDatasetStorageFormat\"])\\\n",
    "            .withColumn(\"description\",dd[\"workspaces\"][\"description\"])\\\n",
    "            .withColumn(\"domainId\",dd[\"workspaces\"][\"domainId\"])\\\n",
    "            .withColumn(\"isOnDedicatedCapacity\",df[\"workspaces\"][\"isOnDedicatedCapacity\"])\\\n",
    "            .withColumn(\"datasets\",explode(df[\"workspaces\"][\"datasets\"]))\n",
    "\n",
    "dd = df.withColumn(\"workspaces\", explode(df.workspaces)).drop(\"datasourceInstances\")\n",
    "display(dd)\n",
    "\n",
    "display(dd.select(dd[\"workspaces\"][\"id\"]))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets\")\n",
    "display(df.limit(10))\n",
    "\n",
    "datasets = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "display(datasets.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"Tables/workspaces_workspaces\"\n",
    "df = spark.read.format(\"delta\").load(ds)\n",
    "dd = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = dd.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(ds)\n",
    "\n",
    "latest_records_df.write.format(\"delta\").save(\"Tables/catalog_datasets_unique\")\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_tables\")\n",
    "df_tables = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_columns\")\n",
    "df_columns = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_measures\")\n",
    "df_measures = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = df_measures.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "tables = df_tables.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "columns = df_columns.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "\n",
    "measures.write.format(\"delta\").save(\"Tables/catalog_datasets_measures_history\")\n",
    "tables.dropDuplicates(\"UID\").write.format(\"delta\").save(\"Tables/catalog_datasets_tables_history\")\n",
    "columns.write.format(\"delta\").save(\"Tables/catalog_datasets_columns_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_columns_history\")\n",
    "display(dd.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_tables_history\")\n",
    "cols = [\"datasets_id\",\"datasets_tables_name\",\"scandate\"]\n",
    "display(tables.orderBy(*cols).select(*cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.dropDuplicates([\"UID\"]).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"True\").save(\"Tables/catalog_datasets_tables_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select T.UID, datasets_tables_name, datasets_id, scandate, description, name FROM catalog_tables_history as T\n",
    "INNER JOIN (select UID, COUNT(UID) FROM catalog_tables_history group by UID having count(UID)>1) as G\n",
    "ON T.UID=G.UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"datasets_id\", \"datasets_tables_name\"]\n",
    "\n",
    "window_spec = Window.partitionBy(*cols).orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = df_tables.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.select(col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
