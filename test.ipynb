{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from env.audit import Audits\n",
    "\n",
    "appsecret = os.getenv(\"admin_appsecret\")\n",
    "appid = os.getenv(\"admin_appid\")\n",
    "tenantid = os.getenv(\"admin_tenantid\")\n",
    "\n",
    "audit = Audits()\n",
    "audit.set_ServicePrincipal(\n",
    "    tenant_id=tenantid,\n",
    "    client_id=appid,\n",
    "    client_secret= appsecret\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the current state {\"activity\": {\"lastRun\": \"2024-09-16T15:57:29.844524Z\"}, \"apps\": {\"lastRun\": \"2024-09-16T15:57:29.844556Z\"}, \"catalog\": {\"lastFullScan\": \"2024-08-11T13:25:04.695307Z\", \"lastRun\": \"2024-09-18T15:18:42.103336Z\"}, \"gateway\": {\"lastRun\": \"2024-09-16T15:57:29.844594Z\"}, \"graph\": {\"lastRun\": \"2024-09-16T15:57:29.844597Z\"}, \"refreshables\": {\"lastRun\": \"2024-09-16T15:57:29.844599Z\"}, \"refreshhistory\": {\"lastRun\": \"2024-09-10T13:54:18.725673Z\"}, \"tenant\": {\"lastRun\": \"2024-09-16T15:57:29.844602Z\"}, \"capacity\": {\"lastRun\": \"2024-09-16T15:57:29.844560Z\"}, \"roles\": {\"lastRun\": \"2024-08-11T13:25:04.695307Z\"}, \"domains\": {\"lastRun\": \"2024-09-16T15:57:29.844566Z\"}, \"fabricitems\": {\"lastRun\": \"2024-09-16T15:57:29.844591Z\"}, \"workspaces\": {\"lastRun\": \"2024-09-16T15:57:29.844604Z\"}}\n",
      "Task Activity is now running\n",
      "What is current state {\"activity\": {\"lastRun\": \"2024-09-16T15:57:29.844524Z\"}, \"apps\": {\"lastRun\": \"2024-09-16T15:57:29.844556Z\"}, \"catalog\": {\"lastFullScan\": \"2024-08-11T13:25:04.695307Z\", \"lastRun\": \"2024-09-18T15:18:42.103336Z\"}, \"gateway\": {\"lastRun\": \"2024-09-16T15:57:29.844594Z\"}, \"graph\": {\"lastRun\": \"2024-09-16T15:57:29.844597Z\"}, \"refreshables\": {\"lastRun\": \"2024-09-16T15:57:29.844599Z\"}, \"refreshhistory\": {\"lastRun\": \"2024-09-10T13:54:18.725673Z\"}, \"tenant\": {\"lastRun\": \"2024-09-16T15:57:29.844602Z\"}, \"capacity\": {\"lastRun\": \"2024-09-16T15:57:29.844560Z\"}, \"roles\": {\"lastRun\": \"2024-08-11T13:25:04.695307Z\"}, \"domains\": {\"lastRun\": \"2024-09-16T15:57:29.844566Z\"}, \"fabricitems\": {\"lastRun\": \"2024-09-16T15:57:29.844591Z\"}, \"workspaces\": {\"lastRun\": \"2024-09-16T15:57:29.844604Z\"}}\n",
      "Task Catalog Snapshots (Published Apps) is now running\n",
      "Task Capacities is now running\n",
      "Task Catalog Scans (Catalog meta data) is now running\n",
      "Task Domains is now running\n",
      "Task Fabric Items is now running\n",
      "Task Gateways is now running\n",
      "Task Gateway elapsed time: 2.9\n",
      "Task Graph is now running\n",
      "Task Gateway elapsed time: 1.8\n",
      "Task Refreshables is now running\n",
      "Task Refresh history is now running\n",
      "Task Tenant Settings is now running\n",
      "Task Refreshables elapsed time: 1.1\n",
      "Task Workspaces in tenant is now running\n",
      "Processing batch number 1 of 1 batches\n",
      "Processing item number 1\n",
      "Task Apps elapsed time: 11.7\n",
      "Task Capacity elapsed time: 13.2\n",
      "Task Gateway elapsed time: 17.0\n",
      "Task Refreshables elapsed time: 15.7\n",
      "Task Domains elapsed time: 25.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error saving file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'NoneType' object has no attribute 'encode'\n",
      "Task FabricItems elapsed time: 39.9\n",
      "Saving scan results to 20240924_00001.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 2\n",
      "Saving scan results to 20240924_00002.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 3\n",
      "Saving scan results to 20240924_00003.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 4\n",
      "Saving scan results to 20240924_00004.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 5\n",
      "Saving scan results to 20240924_00005.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 6\n",
      "Saving scan results to 20240924_00006.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 7\n",
      "Saving scan results to 20240924_00007.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 8\n",
      "Saving scan results to 20240924_00008.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 9\n",
      "Saving scan results to 20240924_00009.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 10\n",
      "Saving scan results to 20240924_00010.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 11\n",
      "Saving scan results to 20240924_00011.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 12\n",
      "Saving scan results to 20240924_00012.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 13\n",
      "Saving scan results to 20240924_00013.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 14\n",
      "Saving scan results to 20240924_00014.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 15\n",
      "Saving scan results to 20240924_00015.scanResults.fullscan.json\n",
      "Saved\n",
      "Processing item number 16\n",
      "Saving scan results to 20240924_00016.scanResults.fullscan.json\n",
      "Saved\n",
      "I sleeping\n",
      "Task Catalog elapsed time: 538.6\n",
      "Task Activity elapsed time: 641.2\n",
      "Task Graph elapsed time: 3476.4\n",
      "\n",
      "Total elapsed time: 3483.8\n"
     ]
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "from env.audit import Audits\n",
    "\n",
    "# from notebookutils import mssparkutils\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "\n",
    "\n",
    "LakehouseName = \"\"\n",
    "WarehouseName = \"\"\n",
    "WorkspaceName = \"\"\n",
    "\n",
    "keyvault_url = \"\"\n",
    "\n",
    "ServicePrincipal = {\n",
    "    \"AppId\" : \"\",\n",
    "    \"AppSecret\" : \"\",\n",
    "    \"TenantId\" : \"\",\n",
    "    \"Environment\" : \"Public\"\n",
    "}\n",
    "\n",
    "audit = Audits()\n",
    "audit.set_ServicePrincipal(\n",
    "    tenant_id=ServicePrincipal['TenantId'],\n",
    "    client_id=ServicePrincipal[\"AppId\"],\n",
    "    client_secret= ServicePrincipal[\"AppSecret\"]\n",
    ")\n",
    "\n",
    "# must have\n",
    "audit.set_LakehouseName(\"LakehouseName\")  \n",
    "audit.set_WorkspaceName(\"WorkspaceName\") \n",
    "\n",
    "# optional\n",
    "audit.set_ApplicationModules(\"Activity,Apps,Capacity,Catalog,Domains,FabricItems,Gateway,Graph,Refreshables,RefreshHistory,Tenant,Workspaces\")\n",
    "# audit.set_ApplicationModules(\"Catalog\")\n",
    "audit.set_PathInLakehouse(\"stage\")\n",
    "audit.set_all_workspaces(False)\n",
    "\n",
    "# set cron for modules \n",
    "audit.set_Activity_cron(\"* * * * * 30\")\n",
    "audit.set_Catalog_cron(\"* * * * * 30\")\n",
    "audit.set_Capacity_cron(\"* * * * * 30\")\n",
    "audit.set_Apps_cron(\"* * * * * 30\")\n",
    "audit.set_Domains_cron(\"* * * * * 30\")\n",
    "audit.set_Graph_cron(\"* * * * * 30\")\n",
    "audit.set_Tenant_cron(\"* * * * * 30\")\n",
    "audit.set_RefreshHistory_cron(\"* * * * * 30\")\n",
    "audit.set_Refreshables_cron(\"* * * * * 30\")\n",
    "audit.set_Gateway_cron(\"* * * * * 30\")\n",
    "\n",
    "# Run the application\n",
    "await audit.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPSERT Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Assuming you have a Delta table named \"my_delta_table\"\n",
    "delta_table_path = \"path/to/delta-table\"  # Replace with your actual path\n",
    "\n",
    "# Create or load your DataFrame (myDataFrame) with the data to be upserted\n",
    "# For demonstration purposes, let's assume you have columns: \"Id\", \"Type\", \"Value\"\n",
    "\n",
    "# Initialize the DeltaTable\n",
    "# delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "delta_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "myDataFrame = spark.read.format(\"delta\").load(myDataFrame_path)\n",
    "\n",
    "# Merge the updates into the Delta table\n",
    "delta_table.alias(\"events\") \\\n",
    "    .merge(myDataFrame.alias(\"updates\"), \"events.Id = updates.Id and events.Type = updates.Type\") \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "# Commit the changes\n",
    "delta_table.toDF().write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.set(\"fs.azure.account.auth.type.onelake.dfs.core.windows.net\", \"OAuth\")\n",
    "sparkConf.set(\"fs.azure.account.oauth.provider.type.onelake.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.id.onelake.dfs.core.windows.net\", appid)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.secret.onelake.dfs.core.windows.net\", appsecret)\n",
    "sparkConf.set(\"fs.azure.account.oauth2.client.endpoint.onelake.dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantid)\n",
    "sparkConf.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.1\")\n",
    "spark = SparkSession.builder.config(conf=sparkConf).appName(\"SparkTest\").getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "data = [(\"John\", \"30\", now), (\"Jane\", \"25\", now)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"source\", lit(\"test\"))\n",
    "\n",
    "df.write.format(\"parquet\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n",
    "# df.write.format(\"parquet\").mode(\"append\").save(\"FabricLake.Lakehouse/Files/data.parquet\")\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.format(\"delta\").mode(\"append\").saveAsTable(\"data\")\n",
    "df.write.format(\"parquet\").save(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a DataLakeServiceClient using a connection string\n",
    "   from azure.storage.filedatalake import DataLakeServiceClient\n",
    "   datalake_service_client = DataLakeServiceClient.from_connection_string(self.connection_string)\n",
    "\n",
    "   # Instantiate a FileSystemClient\n",
    "   file_system_client = datalake_service_client.get_file_system_client(\"mynewfilesystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "\n",
    "counter = int()\n",
    "\n",
    "for i in range(1,84):\n",
    "    counter+=1\n",
    "\n",
    "    if i % 17 == 0:\n",
    "        print(datetime.datetime.now())\n",
    "        sleep(5)\n",
    "        print(f\"FizzBuzz and {counter}\")\n",
    "        # print(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = range(1,15000)\n",
    "\n",
    "def distribute_items(items, num_groups):\n",
    "    # Calculate the size of each group\n",
    "    group_size = len(items) // num_groups\n",
    "    remainder = len(items) % num_groups\n",
    "\n",
    "    # Create the groups\n",
    "    groups = []\n",
    "    start = 0\n",
    "    for i in range(num_groups):\n",
    "        end = start + group_size + (1 if i < remainder else 0)\n",
    "        groups.append(items[start:end])\n",
    "        start = end\n",
    "\n",
    "    return groups\n",
    "\n",
    "# Example usage\n",
    "items = list(range(1, 15001))  # List of 15,000 items\n",
    "num_groups = 16\n",
    "groups = distribute_items(items, num_groups)\n",
    "\n",
    "# Print the size of each group\n",
    "for i, group in enumerate(groups):\n",
    "    print(f\"Group {i+1}: {len(group)} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "storage_account_name = \"onelake\"\n",
    "workspace_name = \"FabricMonitor\"\n",
    "\n",
    "credential = ClientSecretCredential(tenant_id=tenantid,\n",
    "                            client_id=appid,\n",
    "                            client_secret=appsecret)\n",
    "\n",
    "file_system_client = FileSystemClient(\n",
    "    account_url=\"https://onelake.dfs.fabric.microsoft.com\",\n",
    "    file_system_name=workspace_name,\n",
    "    credential=credential)\n",
    "\n",
    "dc = file_system_client.get_directory_client(\"FabricLake.Lakehouse/Files\")\n",
    "file_client = dc.get_file_client(\"data.parquet\")\n",
    "with open(\"./data.parquet\", \"rb\") as data:\n",
    "    file_client.upload_data(data, overwrite=True)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"abfss://onelake.dfs.core.windows.net/FabricLake.Lakehouse/Files/data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load data into pandas DataFrame from \"/lakehouse/default/\" + \"Files/stage/state.json\"\n",
    "state = pd.read_json(\"/lakehouse/default/\" + \"Files/stage/state.yaml\",typ=\"series\")\n",
    "\n",
    "lastRun = state['activity']['lastRun']\n",
    "lastRun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/lakehouse/default/\" + \"Files/stage/state.yaml\", \"r\") as file:\n",
    "    for line in file:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spark.read.option(\"multiline\", \"true\").json(\"Files/stage/state.json\")\n",
    "df = spark.read.option(\"multiLines\",\"json\").json(\"Files/stage/catalog/scans/2024/07/22/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "\n",
    "data = df.withColumn(\"datasets\", explode(df[\"workspaces\"][\"datasets\"]))\n",
    "\n",
    "df = df.withColumn(\"start_Time\", to_utc_timestamp(df['startTime'], \"UTC\"))\n",
    "df = df.withColumn(\"cst_timestamp\", from_utc_timestamp(df.start_Time, \"America/Chicago\"))\n",
    "df = df.withColumn(\"eu_timestamp\", from_utc_timestamp(df.start_Time, \"Europe/Berlin\"))\n",
    "df = df.withColumn(\"ist_timestamp\", from_utc_timestamp(df.start_Time, \"Asia/Kolkata\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "\n",
    "dc = df.withColumn(\"requestId\", df.requestId)\\\n",
    "    .withColumn(\"refreshAttempts\", explode(df.refreshAttempts))\\\n",
    "    .select(\"requestId\",\"refreshAttempts\")\n",
    "\n",
    "display(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = dc.withColumn(\"attemptId\", dc.refreshAttempts.attemptId)\\\n",
    "    .withColumn(\"startTime\", dc.refreshAttempts.startTime)\\\n",
    "    .withColumn(\"endTime\", dc.refreshAttempts.endTime)\\\n",
    "    .withColumn(\"type\", dc.refreshAttempts.type)\\\n",
    "    .select(\"requestId\",\"attemptId\",\"startTime\",\"endTime\",\"type\")\n",
    "\n",
    "display(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('dataset_id', 'dataset_name', 'endTime', 'id', 'refreshType', 'requestId', 'serviceExceptionJson', 'startTime', 'status').dropDuplicates([\"dataset_id\",\"requestId\"]).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/refreshhistory\")\n",
    "de.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/refreshhistory_attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT dataset_id, COUNT(DISTINCT requestId) as requestId_count\n",
    "FROM refreshhistory\n",
    "GROUP BY dataset_id\n",
    "HAVING COUNT(DISTINCT requestId) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the latest record by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"Tables/workspaces_workspaces\"\n",
    "df = spark.read.format(\"delta\").load(ds)\n",
    "dd = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = dd.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiline\",\"true\").json(\"Files/stage/catalog/scans/2024/*/*/*.json\")\n",
    "dd = df.withColumn(\"workspaces\", explode(df.workspaces)).drop(\"datasourceInstances\")\n",
    "\n",
    "workspaces = dd.withColumn(\"capacityId\", dd[\"workspaces\"][\"capacityId\"]) \\\n",
    "    .withColumn(\"dataRetrievalState\", dd[\"workspaces\"][\"dataRetrievalState\"]) \\\n",
    "    .withColumn(\"defaultDatasetStorageFormat\", dd[\"workspaces\"][\"defaultDatasetStorageFormat\"]) \\\n",
    "    .withColumn(\"description\", dd[\"workspaces\"][\"description\"]) \\\n",
    "    .withColumn(\"domainId\", dd[\"workspaces\"][\"domainId\"]) \\\n",
    "    .withColumn(\"id\", dd[\"workspaces\"][\"id\"]) \\\n",
    "    .withColumn(\"isOnDedicatedCapacity\", dd[\"workspaces\"][\"isOnDedicatedCapacity\"]) \\\n",
    "    .withColumn(\"name\", dd[\"workspaces\"][\"name\"]) \\\n",
    "    .withColumn(\"state\", dd[\"workspaces\"][\"state\"]) \\\n",
    "    .withColumn(\"type\", dd[\"workspaces\"][\"type\"]) \n",
    "\n",
    "\n",
    "ds = dd.withColumn(\"id\", dd.workspaces.id)\\\n",
    "    .withColumn(\"datasets\", explode(dd.workspaces.datasets)).drop(\"workspaces\")\n",
    "\n",
    "ds = ds.withColumn(\"dataset_id\", ds[\"datasets\"][\"id\"])\n",
    "\n",
    "ds.select(\"id\",\"dataset_id\").dropDuplicates().write.format(\"delta\").mode(\"overwrite\").save(\"Tables/catalog_workspace_datasets\")\n",
    "\n",
    "ee = spark.read.format(\"delta\").load(\"Tables/datasetrefreshable_datasetrefreshable\")\n",
    "display(ee)\n",
    "\n",
    "workspaces = dd.withColumn(\"id\", dd[\"workspaces\"][\"id\"]) \\\n",
    "            .withColumn(\"defaultDatasetStorageFormat\",dd[\"workspaces\"][\"defaultDatasetStorageFormat\"])\\\n",
    "            .withColumn(\"description\",dd[\"workspaces\"][\"description\"])\\\n",
    "            .withColumn(\"domainId\",dd[\"workspaces\"][\"domainId\"])\\\n",
    "            .withColumn(\"isOnDedicatedCapacity\",df[\"workspaces\"][\"isOnDedicatedCapacity\"])\\\n",
    "            .withColumn(\"datasets\",explode(df[\"workspaces\"][\"datasets\"]))\n",
    "\n",
    "dd = df.withColumn(\"workspaces\", explode(df.workspaces)).drop(\"datasourceInstances\")\n",
    "display(dd)\n",
    "\n",
    "display(dd.select(dd[\"workspaces\"][\"id\"]))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets\")\n",
    "display(df.limit(10))\n",
    "\n",
    "datasets = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "display(datasets.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = \"Tables/workspaces_workspaces\"\n",
    "df = spark.read.format(\"delta\").load(ds)\n",
    "dd = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"id\").orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = dd.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(ds)\n",
    "\n",
    "latest_records_df.write.format(\"delta\").save(\"Tables/catalog_datasets_unique\")\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_tables\")\n",
    "df_tables = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_columns\")\n",
    "df_columns = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n",
    "df = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_measures\")\n",
    "df_measures = df.withColumn(\"scandate\", concat_ws(\"-\",df.ts_year, df.ts_month, df.ts_day).cast(\"date\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = df_measures.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "tables = df_tables.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "columns = df_columns.withColumn(\"UID\", concat_ws(\"-\", col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")))\n",
    "\n",
    "measures.write.format(\"delta\").save(\"Tables/catalog_datasets_measures_history\")\n",
    "tables.dropDuplicates(\"UID\").write.format(\"delta\").save(\"Tables/catalog_datasets_tables_history\")\n",
    "columns.write.format(\"delta\").save(\"Tables/catalog_datasets_columns_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_columns_history\")\n",
    "display(dd.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = spark.read.format(\"delta\").load(\"Tables/catalog_datasets_tables_history\")\n",
    "cols = [\"datasets_id\",\"datasets_tables_name\",\"scandate\"]\n",
    "display(tables.orderBy(*cols).select(*cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.dropDuplicates([\"UID\"]).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"True\").save(\"Tables/catalog_datasets_tables_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "select T.UID, datasets_tables_name, datasets_id, scandate, description, name FROM catalog_tables_history as T\n",
    "INNER JOIN (select UID, COUNT(UID) FROM catalog_tables_history group by UID having count(UID)>1) as G\n",
    "ON T.UID=G.UID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"datasets_id\", \"datasets_tables_name\"]\n",
    "\n",
    "window_spec = Window.partitionBy(*cols).orderBy(col(\"scandate\").desc())\n",
    "\n",
    "# Add row number to each partition\n",
    "df_with_row_num = df_tables.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to get the latest records for each id\n",
    "latest_records_df = df_with_row_num.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
    "\n",
    "# Show the latest records DataFrame\n",
    "latest_records_df.select(col(\"datasets_id\"), col(\"datasets_tables_name\"), col(\"scandate\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
