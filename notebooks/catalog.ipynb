{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from  pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import to_timestamp, from_utc_timestamp\n",
    "\n",
    "remove_tables=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spark.read.option(\"multiline\",\"true\").json(f\"Files/stage/catalog/scans/*/*/*/*.json\").withColumn(\"file_Name\", input_file_name())\n",
    "tmp = tmp.withColumn(\"ts_year\", split(tmp['file_Name'],\"/\")[8])\n",
    "tmp = tmp.withColumn(\"ts_month\", split(tmp['file_Name'],\"/\")[9])\n",
    "tmp = tmp.withColumn(\"ts_day\", split(tmp['file_Name'],\"/\")[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workspaces = tmp \\\n",
    "    .withColumn(\"workspaces\", explode(tmp[\"workspaces\"])) \\\n",
    "    .drop(tmp[\"datasourceInstances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(string):\n",
    "    start = string.find(':') + 1\n",
    "    end = string.find('(')\n",
    "    return string[start:end].strip()\n",
    "\n",
    "def extract_name(string):\n",
    "    start = string.find('--') + 1\n",
    "    end = string.find(':')\n",
    "    return string[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(string):\n",
    "    start = string.find(':') + 1\n",
    "    end = string.find('(')\n",
    "    value = string[start:end].strip()\n",
    "\n",
    "    start_v = string.find('--') + 1\n",
    "    end_v = string.find(':')\n",
    "    key = string[start_v:end_v].split(\"-\")[-1].strip()    \n",
    "\n",
    "    meta = dict()\n",
    "    meta[key]=value\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(df):\n",
    "\n",
    "    lst = df._jdf.schema().treeString()\n",
    "    lines = lst.split(\"\\n\")\n",
    "\n",
    "    artifacts = list()\n",
    "    arrays = list()\n",
    "    root_columns = list()\n",
    "    fields = list()\n",
    "    meta = dict()\n",
    "\n",
    "    for line in lines:\n",
    "        x = line.split(sep=\"|\")\n",
    "        typ = extract_value(line)\n",
    "        name = extract_name(line).split(\"-\")[-1].strip()\n",
    "\n",
    "        if len(x)==2:\n",
    "            if typ not in ['struct','array']:\n",
    "                fields.append(name)\n",
    "                q = get_meta(line)\n",
    "                meta[name]=q[name]\n",
    "        if len(x)==3:\n",
    "            if typ not in ['struct','array']:\n",
    "                fields.append(name)\n",
    "                q = get_meta(line)\n",
    "                meta[name]=q[name]\n",
    "            else:\n",
    "                if typ == 'array':\n",
    "                    arrays.append(name)\n",
    "\n",
    "\n",
    "    return (arrays,fields,meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType  \n",
    "\n",
    "def flatten(schema, prefix=None):\n",
    "    fields = []\n",
    "    for field in schema.fields:\n",
    "        name = prefix + '.' + field.name if prefix else field.name\n",
    "        dtype = field.dataType\n",
    "        if isinstance(dtype, ArrayType):\n",
    "            dtype = dtype.elementType\n",
    "\n",
    "        if isinstance(dtype, StructType):\n",
    "            fields += flatten(dtype, prefix=name)\n",
    "        else:\n",
    "            fields.append(name)\n",
    "\n",
    "    return fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(lst):\n",
    "    duplicates = {}\n",
    "    for index, value in enumerate(lst):\n",
    "        if value in duplicates:\n",
    "            duplicates[value].append(index)\n",
    "        else:\n",
    "            duplicates[value] = [index]\n",
    "    return {key: value for key, value in duplicates.items() if len(value) > 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ( \n",
    "    StringType, BooleanType, IntegerType, FloatType, DateType, LongType, TimestampType, BinaryType, DoubleType \n",
    ") \n",
    "  \n",
    "\n",
    "dataTypes = dict(\n",
    "{\n",
    "    \"string\":\tStringType(),\t\n",
    "    \"integer\":\tIntegerType(),\t\n",
    "    \"long\":\t    LongType(),\t\n",
    "    \"float\":\tFloatType(),\t\n",
    "    \"double\":\tDoubleType(),\t\n",
    "    \"boolean\":\tBooleanType(),\t\n",
    "    \"date\":\t    DateType(),\t\n",
    "    \"timestamp\":TimestampType(),\n",
    "    \"binary\":\tBinaryType()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"bronze/catalog\"\n",
    "nxt = \"Silver/catalog\"\n",
    "save_df = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subitems = list()\n",
    "\n",
    "lst = df_workspaces._jdf.schema().treeString()\n",
    "\n",
    "lines = lst.split(\"\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    x = line.split(sep=\"|\")\n",
    "    typ = extract_value(line)\n",
    "    name = extract_name(line).split(\"-\")[-1].strip()\n",
    "\n",
    "    if len(x)==3:\n",
    "        if typ == \"array\":\n",
    "            subitems.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = df_workspaces.withColumn(\"datasets\", explode(df_workspaces[\"workspaces\"][\"datasets\"])).drop(\"workspaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "workspace = df_workspaces\n",
    "ww = build_df(workspace)\n",
    "\n",
    "for field in ww[1]:\n",
    "    if field not in [\"file_Name\",\"ts_year\",\"ts_month\",\"ts_day\"]:\n",
    "        workspace = workspace.withColumn(field, workspace['workspaces'][field])\n",
    "\n",
    "workspace = workspace.drop(\"workspaces\")\n",
    "workspace.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/workspaces\")\n",
    "\n",
    "for item in subitems:\n",
    "    # if item not in \"datasets\": continue\n",
    "    dp = df_workspaces.withColumn(\"dp\", explode(df_workspaces[\"workspaces\"][item]))\\\n",
    "        .withColumn(\"workspaceId\", df_workspaces[\"workspaces\"][\"id\"])\\\n",
    "        .drop(\"workspaces\") \n",
    "\n",
    "    df = build_df(dp)\n",
    "\n",
    "    for field in df[1]:\n",
    "        meta = df[2]\n",
    "        if field not in dp.columns:\n",
    "            dp = dp.withColumn(field, dp['dp'][field].cast(meta[field]))\n",
    "\n",
    "    if item not in \"dataflows\":\n",
    "        pkfk_1 = f\"{item}id\"\n",
    "        dp = dp.withColumnRenamed(\"id\", pkfk_1 )\n",
    "        dp = dp.withColumn(\"UID\", concat(col(f\"{item}id\"), lit(\"-\"), col(\"ts_year\"), col(\"ts_month\"),col(\"ts_day\")))\n",
    "    else:\n",
    "        pkfk_1 = f\"{item}objectid\"\n",
    "        dp = dp.withColumnRenamed(\"objectid\", pkfk_1 )\n",
    "        dp = dp.withColumn(\"UID\", concat(col(f\"{item}objectid\"), lit(\"-\"), col(\"ts_year\"), col(\"ts_month\"),col(\"ts_day\")))\n",
    "\n",
    "\n",
    "    if save_df:\n",
    "        print(f\"saving...Files/{test}/{item}\")\n",
    "        dp.drop(\"dp\").write.format(\"parquet\").mode(\"overwrite\").save(f\"Files/{test}/{item}\")    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Working on item: {item}\")\n",
    "    dataframes = dict()\n",
    "\n",
    "\n",
    "    # make the sub dataframes of the parent\n",
    "    for array in df[0]:\n",
    "        id = \"id\"\n",
    "        if item in \"dataflows\":\n",
    "            id = \"objectId\"\n",
    "        elif array in \"tables\":\n",
    "            id = \"id\" # \"name\"\n",
    "        elif item == \"dataflows\" and array == \"relations\":            \n",
    "            id = \"objectId\"\n",
    "        elif item in \"dataflows\" and array in \"datasourceUsages\":\n",
    "            # id = \"datasourceInstanceId\"\n",
    "            id = \"objectId\"\n",
    "\n",
    "        print(f\"what array are we using:{array}\")\n",
    "       \n",
    "        dataframes[array] = dp.withColumn(array, explode(dp[\"dp\"][array]))\\\n",
    "                .withColumn(f\"{item}id\", dp[pkfk_1])\\\n",
    "                .select(pkfk_1,\"ts_year\", \"ts_month\", \"ts_day\", array)\n",
    "\n",
    "        dataframes[array] = dataframes[array].withColumn(f\"{item}_UID\",concat(col(pkfk_1),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\n",
    "        if array in [\"roles\",\"tables\"]:\n",
    "            print(\"i have entered the other side before\")\n",
    "            display(dataframes[array])\n",
    "            dataframes[array] = dataframes[array].withColumn(f\"{array}_UID\", concat(col(f\"{item}_UID\"),lit(\"-\"),dataframes[array][array][\"name\"]))\n",
    "            print(\"i have entered the other side after\")                  \n",
    "            display(dataframes[array])\n",
    "            \n",
    "        \n",
    "\n",
    "        # check for any arrays in the sub items\n",
    "        df2 = build_df(dataframes[array])\n",
    "        print(\"what is the extra\",df2[0])\n",
    "\n",
    "        if df2[0]:\n",
    "            dataframes2 = dict()\n",
    "\n",
    "            # display(dataframes[array])\n",
    "            for array2 in df2[0]:\n",
    "                \n",
    "                tt = dataframes[array].withColumn(array2, explode(dataframes[array][array][array2]))\n",
    "                tt = tt.withColumn(f\"{array}_UID\", concat(f\"{item}_UID\",lit(\"-\"),tt[array][\"name\"]))\n",
    "                tt = tt.drop(array)\n",
    "                tt = tt.select(flatten(tt.schema))     \n",
    "                # display(tt.limit(5))\n",
    "\n",
    "                # some of the children tables have the same name as the parent. The children columns\n",
    "                # will be appended with their index from the columns\n",
    "                dups = find_duplicates(tt.columns)\n",
    "                if dups:\n",
    "                    for k in dups:\n",
    "                        for i in range(1,len(dups[k])):\n",
    "                            idx = dups[k][i]\n",
    "\n",
    "                            # there isn't a way to do this with pyspark as renamed columns will in our case find the name of the column and rename it\n",
    "                            # but we have duplicate column names and this will rename both the columns and not the column at the indexed position\n",
    "                            # however, pandas does have that feature so we can rename a column at a specific index\n",
    "\n",
    "                            t = tt.toPandas()\n",
    "                            t.columns.values[idx]=f\"{array2}_{k}\"\n",
    "                            tt = spark.createDataFrame(t)\n",
    "\n",
    "                            if save_df:\n",
    "                                print(f\"saving...sub-sub items...Files/{test}/{item}/{array}/{array2}\")\n",
    "                                try:\n",
    "                                    # check to see if the folder exists\n",
    "                                    # if it does then append the files to the folder                            \n",
    "                                    item = mssparkutils.fs.ls(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                                    tt.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                                except Exception as e:\n",
    "                                    pass\n",
    "                                    tt.write.format(\"parquet\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                else:\n",
    "                    if save_df:\n",
    "                        print(f\"saving...sub sub-items...Files/{test}/{item}/{array}/{array2}\")\n",
    "                        try:\n",
    "                            # check to see if the folder exists\n",
    "                            # if it does then append the files to the folder                            \n",
    "                            item = mssparkutils.fs.ls(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                            tt.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                            tt.write.format(\"parquet\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "\n",
    "                del(tt)\n",
    "\n",
    "\n",
    "        # drop each sub dataframe from the parent\n",
    "        dp = dp.drop(array)\n",
    "\n",
    "\n",
    "    tmp = dict()\n",
    "\n",
    "    for key in dataframes:\n",
    "        print(f\"what is the key {key}\")\n",
    "        output = build_df(dataframes[key])\n",
    "        for field in output[1]:\n",
    "            if field not in dataframes[key].columns:\n",
    "                dataframes[key] = dataframes[key].withColumn(field, dataframes[key][key][field])\n",
    "\n",
    "        dataframes[key] = dataframes[key].drop(key)\n",
    "        if save_df:\n",
    "            print(f\"saving...sub items...Files/{test}/{item}/{key}\")\n",
    "            dataframes[key].write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{key}\")\n",
    "    \n",
    "    del(dataframes)\n",
    "    del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in mssparkutils.fs.ls(f\"Files/{nxt}\"):\n",
    "    if \"_SUCCESS\" in f.name or f.name.endswith(\"parquet\"):continue\n",
    "    try:\n",
    "        df = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}\")\n",
    "\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{f.name} Part 1 threw an error {e} for {f.name}\")\n",
    "\n",
    "    for ff in mssparkutils.fs.ls(f\"Files/{nxt}/{f.name}\"):\n",
    "        if \"_SUCCESS\" in ff.name or ff.name.endswith(\"parquet\"):continue\n",
    "        try:\n",
    "            dff = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}/{ff.name}\")\n",
    "\n",
    "            dff.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}_{ff.name}\")\n",
    "        except Exception as ee:\n",
    "            print(f\"{ff.name} threw an error {ee}\")\n",
    "\n",
    "        for fff in mssparkutils.fs.ls(f\"Files/{nxt}/{f.name}/{ff.name}\"):\n",
    "            if \"_SUCCESS\" in fff.name or fff.name.endswith(\"parquet\"):continue\n",
    "            try:\n",
    "                dfff = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}/{ff.name}/{fff.name}\")\n",
    "                dfff.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}_{ff.name}_{fff.name}\")\n",
    "            except Exception as eee:\n",
    "                print(f\"{fff.name} threw an error {eee}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = spark.read.option(\"multiline\",\"true\").json(\"Files/stage/datasetrefresh/*/*/*/*.json\").withColumn(\"file_Name\", input_file_name())\n",
    "\n",
    "dr = dr.withColumn(\"ts_year\", split(dr['file_Name'],\"/\")[7])\\\n",
    "    .withColumn(\"ts_month\", split(dr['file_Name'],\"/\")[8])\\\n",
    "    .withColumn(\"ts_day\", split(dr['file_Name'],\"/\")[9])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dr.select(\"dataset_id\",\"refreshAttempts\",\"ts_year\",\"ts_month\",\"ts_day\")\n",
    "r = r.withColumn(\"refreshAttemptsA\", explode(r[\"refreshAttempts\"]))\n",
    "\n",
    "r = r.withColumn(\"startTime\",r[\"refreshAttemptsA\"][\"startTime\"])\\\n",
    "    .withColumn(\"endTime\",r[\"refreshAttemptsA\"][\"endTime\"])\\\n",
    "    .withColumn(\"attemptId\",r[\"refreshAttemptsA\"][\"attemptId\"])\\\n",
    "    .withColumn(\"type\",r[\"refreshAttemptsA\"][\"type\"])\\\n",
    "    .withColumn(\"serviceExceptionJson\",r[\"refreshAttemptsA\"][\"serviceExceptionJson\"])\\\n",
    "    .withColumn(\"dataset_UID\", concat(col(\"dataset_id\"),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\\\n",
    "    .drop(\"refreshAttempts\",\"refreshAttemptsA\")\n",
    "\n",
    "r = r.withColumn(\"errorCode\", split(split(r[\"serviceExceptionJson\"],\",\")[0],\":\")[1])\\\n",
    "    .withColumn(\"errorDescription\", split(split(r[\"serviceExceptionJson\"],\",\")[1],\":\")[1])\\\n",
    "    .drop(\"serviceExceptionJson\")\n",
    "\n",
    "cols = [\"startTime\",\"endTime\"]\n",
    "\n",
    "for c in cols:\n",
    "    r = r.withColumn(f\"{c}_UTC\", from_utc_timestamp(c, \"UTC\"))\n",
    "\n",
    "\n",
    "r.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/datasetRefresh_attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "dr = dr.withColumn(\"dataset_UID\", concat(col(\"dataset_id\"),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\n",
    "dr = dr.drop(\"refreshAttempts\",\"serviceExceptionJson\")\n",
    "\n",
    "cols = [\"startTime\",\"endTime\"]\n",
    "\n",
    "for c in cols:\n",
    "    dr = dr.withColumn(f\"{c}_UTC\", from_utc_timestamp(c, \"UTC\"))\n",
    "\n",
    "dr = dr.withColumn(\"startDate\",date_format(col(\"startTime_UTC\"),\"yyyyMMdd\"))\\\n",
    "    .withColumn(\"startTime\", date_format(col(\"startTime_UTC\"), \"h:mm:ss a\"))\n",
    "\n",
    "dr.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/datasetRefresh\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
