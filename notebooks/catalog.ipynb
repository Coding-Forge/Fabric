{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import to_timestamp, from_utc_timestamp\n",
    "\n",
    "remove_tables=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobClient, BlobServiceClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "\n",
    "def read_from_file(blob_name):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(os.getenv(\"STORAGE_ACCOUNT_CONNECTION_STRING\"))\n",
    "    blob_client = blob_service_client.get_blob_client(container=os.getenv(\"STORAGE_ACCOUNT_CONTAINER_NAME\"), blob=blob_name)\n",
    "    try:\n",
    "        blob_content = blob_client.download_blob().readall()\n",
    "        return blob_content \n",
    "    except Exception as e:\n",
    "        print(f\"Blob not found: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stage/catalog/scans/2025/07/15/20250715_00001.scanResults.json']\n"
     ]
    }
   ],
   "source": [
    "def list_blobs_in_path(path):\n",
    "    container_client = blob_service_client.get_container_client(STORAGE_ACCOUNT_CONTAINER_NAME)\n",
    "    blob_list = container_client.list_blobs(name_starts_with=path)\n",
    "    return [blob.name for blob in blob_list]\n",
    "\n",
    "# Example usage:\n",
    "blobs_in_path = list_blobs_in_path(\"stage/catalog/scans/2025/07/15/\")\n",
    "print(blobs_in_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = read_from_file(blobs_in_path[0])  # Assuming blobs_in_path is not empty and contains the path to the JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workspaceId</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>state</th>\n",
       "      <th>isOnDedicatedCapacity</th>\n",
       "      <th>capacityId</th>\n",
       "      <th>defaultDatasetStorageFormat</th>\n",
       "      <th>reports</th>\n",
       "      <th>dashboards</th>\n",
       "      <th>...</th>\n",
       "      <th>Reflex</th>\n",
       "      <th>Notebook</th>\n",
       "      <th>warehouses</th>\n",
       "      <th>SQLAnalyticsEndpoint</th>\n",
       "      <th>MLExperiment</th>\n",
       "      <th>MLModel</th>\n",
       "      <th>Eventstream</th>\n",
       "      <th>KQLDashboard</th>\n",
       "      <th>KQLDatabase</th>\n",
       "      <th>Eventhouse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a13a9f8a-0469-4efb-96c5-0be69ba5e83b</td>\n",
       "      <td>fill-in-Something-New</td>\n",
       "      <td>Admin workspace for admin reports</td>\n",
       "      <td>Workspace</td>\n",
       "      <td>Active</td>\n",
       "      <td>True</td>\n",
       "      <td>98E82059-755F-4163-AC49-B21C5E9243D3</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cb79cb87-3582-4e2c-bca3-fbd68f68d2da</td>\n",
       "      <td>fill-in-FabricMonitor</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Workspace</td>\n",
       "      <td>Active</td>\n",
       "      <td>True</td>\n",
       "      <td>98E82059-755F-4163-AC49-B21C5E9243D3</td>\n",
       "      <td>Small</td>\n",
       "      <td>[{'reportType': 'PowerBIReport', 'id': '0bde31...</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': '1b5f317a-637f-49f2-97a1-fda1996a2b2a'...</td>\n",
       "      <td>[{'id': 'e9d24c5b-c4bf-4642-8722-99e63727ed45'...</td>\n",
       "      <td>[{'id': '14f359f5-7d2d-42e0-9299-e74646bc7539'...</td>\n",
       "      <td>[{'id': 'd7d5d78b-18da-439b-a8a4-231c791f381f'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            workspaceId                   name  \\\n",
       "0  a13a9f8a-0469-4efb-96c5-0be69ba5e83b  fill-in-Something-New   \n",
       "1  cb79cb87-3582-4e2c-bca3-fbd68f68d2da  fill-in-FabricMonitor   \n",
       "\n",
       "                         description       type   state  \\\n",
       "0  Admin workspace for admin reports  Workspace  Active   \n",
       "1                                NaN  Workspace  Active   \n",
       "\n",
       "   isOnDedicatedCapacity                            capacityId  \\\n",
       "0                   True  98E82059-755F-4163-AC49-B21C5E9243D3   \n",
       "1                   True  98E82059-755F-4163-AC49-B21C5E9243D3   \n",
       "\n",
       "  defaultDatasetStorageFormat  \\\n",
       "0                     Unknown   \n",
       "1                       Small   \n",
       "\n",
       "                                             reports dashboards  ...  \\\n",
       "0                                                 []         []  ...   \n",
       "1  [{'reportType': 'PowerBIReport', 'id': '0bde31...         []  ...   \n",
       "\n",
       "                                              Reflex  \\\n",
       "0                                                NaN   \n",
       "1  [{'id': '1b5f317a-637f-49f2-97a1-fda1996a2b2a'...   \n",
       "\n",
       "                                            Notebook  \\\n",
       "0                                                NaN   \n",
       "1  [{'id': 'e9d24c5b-c4bf-4642-8722-99e63727ed45'...   \n",
       "\n",
       "                                          warehouses  \\\n",
       "0                                                NaN   \n",
       "1  [{'id': '14f359f5-7d2d-42e0-9299-e74646bc7539'...   \n",
       "\n",
       "                                SQLAnalyticsEndpoint MLExperiment MLModel  \\\n",
       "0                                                NaN          NaN     NaN   \n",
       "1  [{'id': 'd7d5d78b-18da-439b-a8a4-231c791f381f'...          NaN     NaN   \n",
       "\n",
       "  Eventstream KQLDashboard KQLDatabase Eventhouse  \n",
       "0         NaN          NaN         NaN        NaN  \n",
       "1         NaN          NaN         NaN        NaN  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Convert binary JSON data to string\n",
    "json_str = blobs.decode(\"utf-8\")\n",
    "# Parse JSON string to Python object\n",
    "json_obj = json.loads(json_str)\n",
    "\n",
    "# If the JSON is a list of records, create DataFrame directly\n",
    "if isinstance(json_obj, list):\n",
    "    df = spark.createDataFrame(json_obj)\n",
    "else:\n",
    "    # If it's a dict, wrap in a list\n",
    "    dfs = []\n",
    "    for workspace in json_obj[\"workspaces\"]:\n",
    "        df = pd.json_normalize([workspace])\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_df = final_df.rename(columns={\"id\": \"workspaceId\"})\n",
    "display(final_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "# Example: Flatten the 'datasets' column (which contains JSON/list data) into a new DataFrame\n",
    "# Replace 'datasets' with the column you want to flatten\n",
    "\n",
    "# Select the column with JSON/list data\n",
    "def flatten_json_column(df, json_col, parent_id_col):\n",
    "    \"\"\"\n",
    "    Flattens a column containing lists of dicts in a DataFrame and attaches the parent id column.\n",
    "    \"\"\"\n",
    "    if json_col in df.columns and parent_id_col in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        exploded = df[[json_col, parent_id_col]].explode(json_col)\n",
    "        exploded = exploded[exploded[json_col].notna()]\n",
    "        if not exploded.empty:\n",
    "            normalized = pd.json_normalize(exploded[json_col])\n",
    "            normalized[parent_id_col] = exploded[parent_id_col].values\n",
    "            return normalized\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# # Example usage:\n",
    "# # normalized = flatten_json_column(final_df, json_col, 'workspaceId')\n",
    "# # display(normalized.head(2))\n",
    "# # Explode the column if it's a list of dicts\n",
    "# if json_col in final_df.columns:\n",
    "#     final_df = final_df.reset_index(drop=True)\n",
    "#     exploded = final_df[[json_col, 'workspaceId']].explode(json_col)\n",
    "#     exploded = exploded[exploded[json_col].notna()]\n",
    "#     if not exploded.empty:\n",
    "#         normalized = pd.json_normalize(exploded[json_col])\n",
    "#         normalized['workspaceId'] = exploded['workspaceId'].values\n",
    "#         display(normalized.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>isHidden</th>\n",
       "      <th>storageMode</th>\n",
       "      <th>columns</th>\n",
       "      <th>measures</th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tenant</td>\n",
       "      <td>False</td>\n",
       "      <td>DirectLake</td>\n",
       "      <td>[{'name': 'canSpecifySecurityGroups', 'dataTyp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'expression': 'tenant', 'schemaName': 'monit...</td>\n",
       "      <td>6e083520-f9d6-47b5-b289-3bc95739d7f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>activity</td>\n",
       "      <td>False</td>\n",
       "      <td>DirectLake</td>\n",
       "      <td>[{'name': 'Activity', 'dataType': 'String', 'i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{'expression': 'activity', 'schemaName': 'dbo'}]</td>\n",
       "      <td>6e083520-f9d6-47b5-b289-3bc95739d7f4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name  isHidden storageMode  \\\n",
       "0    tenant     False  DirectLake   \n",
       "1  activity     False  DirectLake   \n",
       "\n",
       "                                             columns measures  \\\n",
       "0  [{'name': 'canSpecifySecurityGroups', 'dataTyp...       []   \n",
       "1  [{'name': 'Activity', 'dataType': 'String', 'i...       []   \n",
       "\n",
       "                                              source  \\\n",
       "0  [{'expression': 'tenant', 'schemaName': 'monit...   \n",
       "1  [{'expression': 'activity', 'schemaName': 'dbo'}]   \n",
       "\n",
       "                                     id  \n",
       "0  6e083520-f9d6-47b5-b289-3bc95739d7f4  \n",
       "1  6e083520-f9d6-47b5-b289-3bc95739d7f4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datasets = flatten_json_column(final_df, 'datasets', 'workspaceId')\n",
    "tables = flatten_json_column(datasets, 'tables', 'id')\n",
    "display(tables.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_telemetry: sempy.relationships\n",
      "log_telemetry: sempy.dependencies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'func': 'sempy.fabric._flat.list_datasets', 'total_seconds': 0.143}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/_utils/_log.py\", line 371, in log_decorator_wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_flat.py\", line 203, in list_datasets\n",
      "    return _get_or_create_workspace_client(workspace).get_datasets(mode, additional_xmla_properties)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_cache.py\", line 32, in _get_or_create_workspace_client\n",
      "    client = WorkspaceClient(workspace)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_client/_workspace_client.py\", line 62, in __init__\n",
      "    _init_analysis_services()\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_client/_utils.py\", line 43, in _init_analysis_services\n",
      "    _init_dotnet_runtime(runtime_config, assemblies=assemblies)\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/_utils/_dotnet.py\", line 16, in _init_dotnet_runtime\n",
      "    set_runtime(get_coreclr(runtime_config=os.fspath(config)))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/clr_loader/__init__.py\", line 121, in get_coreclr\n",
      "    dotnet_root = find_dotnet_root()\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brandon/miniconda3/envs/nbks/lib/python3.12/site-packages/clr_loader/util/find.py\", line 57, in find_dotnet_root\n",
      "    raise RuntimeError(\"Can not determine dotnet root\")\n",
      "RuntimeError: Can not determine dotnet root\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_telemetry: sempy.fabric\n",
      "log_telemetry: sempy\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can not determine dotnet root",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# List datasets in your workspace\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df_datasets = fabric.list_datasets()\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_datasets)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Replace with your dataset name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/_utils/_log.py:371\u001b[39m, in \u001b[36mmds_log.<locals>.get_wrapper.<locals>.log_decorator_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    368\u001b[39m start_time = time.perf_counter()\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     result = func(*args, **kwargs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;66;03m# The invocation for get_message_dict moves after the function\u001b[39;00m\n\u001b[32m    374\u001b[39m     \u001b[38;5;66;03m# so it can access the state after the method call\u001b[39;00m\n\u001b[32m    375\u001b[39m     message.update(extractor.get_completion_message_dict(result, arg_dict))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_flat.py:203\u001b[39m, in \u001b[36mlist_datasets\u001b[39m\u001b[34m(workspace, mode, additional_xmla_properties)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;129m@log\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_datasets\u001b[39m(workspace: Optional[Union[\u001b[38;5;28mstr\u001b[39m, UUID]] = \u001b[38;5;28;01mNone\u001b[39;00m, mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mxmla\u001b[39m\u001b[33m\"\u001b[39m, additional_xmla_properties: Optional[Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> pd.DataFrame:\n\u001b[32m    177\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[33;03m    List datasets in a `Fabric workspace <https://learn.microsoft.com/en-us/fabric/get-started/workspaces>`_.\u001b[39;00m\n\u001b[32m    179\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m \u001b[33;03m        Dataframe listing databases and their attributes.\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_or_create_workspace_client(workspace).get_datasets(mode, additional_xmla_properties)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_cache.py:32\u001b[39m, in \u001b[36m_get_or_create_workspace_client\u001b[39m\u001b[34m(workspace)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m workspace \u001b[38;5;129;01min\u001b[39;00m _workspace_clients:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _workspace_clients[workspace]\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m client = WorkspaceClient(workspace)\n\u001b[32m     33\u001b[39m _workspace_clients[client.get_workspace_name()] = client\n\u001b[32m     34\u001b[39m _workspace_clients[client.get_workspace_id()] = client\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_client/_workspace_client.py:62\u001b[39m, in \u001b[36mWorkspaceClient.__init__\u001b[39m\u001b[34m(self, workspace, token_provider)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, workspace: Optional[Union[\u001b[38;5;28mstr\u001b[39m, UUID]] = \u001b[38;5;28;01mNone\u001b[39;00m, token_provider: Optional[TokenProvider] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     _init_analysis_services()\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.token_provider = token_provider \u001b[38;5;129;01mor\u001b[39;00m SynapseTokenProvider()\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mself\u001b[39m._pbi_rest_api = _PBIRestAPI(token_provider=\u001b[38;5;28mself\u001b[39m.token_provider)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/fabric/_client/_utils.py:43\u001b[39m, in \u001b[36m_init_analysis_services\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     31\u001b[39m assembly_path = sempy_root / \u001b[33m\"\u001b[39m\u001b[33mlib\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m assemblies = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m     34\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m assembly_file: assembly_path / assembly_file,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     )\n\u001b[32m     41\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m _init_dotnet_runtime(runtime_config, assemblies=assemblies)\n\u001b[32m     44\u001b[39m _analysis_services_initialized = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/sempy/_utils/_dotnet.py:16\u001b[39m, in \u001b[36m_init_dotnet_runtime\u001b[39m\u001b[34m(config, assemblies)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpythonnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_runtime, get_runtime_info\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_runtime_info() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     set_runtime(get_coreclr(runtime_config=os.fspath(config)))\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assemblies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/clr_loader/__init__.py:121\u001b[39m, in \u001b[36mget_coreclr\u001b[39m\u001b[34m(runtime_config, dotnet_root, properties, runtime_spec)\u001b[39m\n\u001b[32m    119\u001b[39m dotnet_root = _maybe_path(dotnet_root)\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dotnet_root \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     dotnet_root = find_dotnet_root()\n\u001b[32m    123\u001b[39m temp_dir = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    124\u001b[39m runtime_config = _maybe_path(runtime_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/nbks/lib/python3.12/site-packages/clr_loader/util/find.py:57\u001b[39m, in \u001b[36mfind_dotnet_root\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m dotnet_cli = find_dotnet_cli()\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dotnet_cli:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not determine dotnet root\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dotnet_cli.resolve().parent\n",
      "\u001b[31mRuntimeError\u001b[39m: Can not determine dotnet root"
     ]
    }
   ],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "\n",
    "# List datasets in your workspace\n",
    "df_datasets = fabric.list_datasets()\n",
    "print(df_datasets)\n",
    "\n",
    "# Replace with your dataset name\n",
    "dataset_name = \"fill-in-Microsoft Fabric Capacity Metrics\"\n",
    "\n",
    "# List tables in the dataset\n",
    "df_tables = fabric.list_tables(dataset_name, include_columns=True)\n",
    "print(df_tables)\n",
    "\n",
    "df = fabric.read_table(dataset=dataset_name, table=\"MetricsByItemandOperationandDay\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spark.read.option(\"multiline\",\"true\").json(f\"Files/stage/catalog/scans/*/*/*/*.json\").withColumn(\"file_Name\", input_file_name())\n",
    "tmp = tmp.withColumn(\"ts_year\", split(tmp['file_Name'],\"/\")[8])\n",
    "tmp = tmp.withColumn(\"ts_month\", split(tmp['file_Name'],\"/\")[9])\n",
    "tmp = tmp.withColumn(\"ts_day\", split(tmp['file_Name'],\"/\")[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workspaces = tmp \\\n",
    "    .withColumn(\"workspaces\", explode(tmp[\"workspaces\"])) \\\n",
    "    .drop(tmp[\"datasourceInstances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(string):\n",
    "    start = string.find(':') + 1\n",
    "    end = string.find('(')\n",
    "    return string[start:end].strip()\n",
    "\n",
    "def extract_name(string):\n",
    "    start = string.find('--') + 1\n",
    "    end = string.find(':')\n",
    "    return string[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(string):\n",
    "    start = string.find(':') + 1\n",
    "    end = string.find('(')\n",
    "    value = string[start:end].strip()\n",
    "\n",
    "    start_v = string.find('--') + 1\n",
    "    end_v = string.find(':')\n",
    "    key = string[start_v:end_v].split(\"-\")[-1].strip()    \n",
    "\n",
    "    meta = dict()\n",
    "    meta[key]=value\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(df):\n",
    "\n",
    "    lst = df._jdf.schema().treeString()\n",
    "    lines = lst.split(\"\\n\")\n",
    "\n",
    "    artifacts = list()\n",
    "    arrays = list()\n",
    "    root_columns = list()\n",
    "    fields = list()\n",
    "    meta = dict()\n",
    "\n",
    "    for line in lines:\n",
    "        x = line.split(sep=\"|\")\n",
    "        typ = extract_value(line)\n",
    "        name = extract_name(line).split(\"-\")[-1].strip()\n",
    "\n",
    "        if len(x)==2:\n",
    "            if typ not in ['struct','array']:\n",
    "                fields.append(name)\n",
    "                q = get_meta(line)\n",
    "                meta[name]=q[name]\n",
    "        if len(x)==3:\n",
    "            if typ not in ['struct','array']:\n",
    "                fields.append(name)\n",
    "                q = get_meta(line)\n",
    "                meta[name]=q[name]\n",
    "            else:\n",
    "                if typ == 'array':\n",
    "                    arrays.append(name)\n",
    "\n",
    "\n",
    "    return (arrays,fields,meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType  \n",
    "\n",
    "def flatten(schema, prefix=None):\n",
    "    fields = []\n",
    "    for field in schema.fields:\n",
    "        name = prefix + '.' + field.name if prefix else field.name\n",
    "        dtype = field.dataType\n",
    "        if isinstance(dtype, ArrayType):\n",
    "            dtype = dtype.elementType\n",
    "\n",
    "        if isinstance(dtype, StructType):\n",
    "            fields += flatten(dtype, prefix=name)\n",
    "        else:\n",
    "            fields.append(name)\n",
    "\n",
    "    return fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(lst):\n",
    "    duplicates = {}\n",
    "    for index, value in enumerate(lst):\n",
    "        if value in duplicates:\n",
    "            duplicates[value].append(index)\n",
    "        else:\n",
    "            duplicates[value] = [index]\n",
    "    return {key: value for key, value in duplicates.items() if len(value) > 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ( \n",
    "    StringType, BooleanType, IntegerType, FloatType, DateType, LongType, TimestampType, BinaryType, DoubleType \n",
    ") \n",
    "  \n",
    "\n",
    "dataTypes = dict(\n",
    "{\n",
    "    \"string\":\tStringType(),\t\n",
    "    \"integer\":\tIntegerType(),\t\n",
    "    \"long\":\t    LongType(),\t\n",
    "    \"float\":\tFloatType(),\t\n",
    "    \"double\":\tDoubleType(),\t\n",
    "    \"boolean\":\tBooleanType(),\t\n",
    "    \"date\":\t    DateType(),\t\n",
    "    \"timestamp\":TimestampType(),\n",
    "    \"binary\":\tBinaryType()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"bronze/catalog\"\n",
    "nxt = \"Silver/catalog\"\n",
    "save_df = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subitems = list()\n",
    "\n",
    "lst = df_workspaces._jdf.schema().treeString()\n",
    "\n",
    "lines = lst.split(\"\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    x = line.split(sep=\"|\")\n",
    "    typ = extract_value(line)\n",
    "    name = extract_name(line).split(\"-\")[-1].strip()\n",
    "\n",
    "    if len(x)==3:\n",
    "        if typ == \"array\":\n",
    "            subitems.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = df_workspaces.withColumn(\"datasets\", explode(df_workspaces[\"workspaces\"][\"datasets\"])).drop(\"workspaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "workspace = df_workspaces\n",
    "ww = build_df(workspace)\n",
    "\n",
    "for field in ww[1]:\n",
    "    if field not in [\"file_Name\",\"ts_year\",\"ts_month\",\"ts_day\"]:\n",
    "        workspace = workspace.withColumn(field, workspace['workspaces'][field])\n",
    "\n",
    "workspace = workspace.drop(\"workspaces\")\n",
    "workspace.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/workspaces\")\n",
    "\n",
    "for item in subitems:\n",
    "    # if item not in \"datasets\": continue\n",
    "    dp = df_workspaces.withColumn(\"dp\", explode(df_workspaces[\"workspaces\"][item]))\\\n",
    "        .withColumn(\"workspaceId\", df_workspaces[\"workspaces\"][\"id\"])\\\n",
    "        .drop(\"workspaces\") \n",
    "\n",
    "    df = build_df(dp)\n",
    "\n",
    "    for field in df[1]:\n",
    "        meta = df[2]\n",
    "        if field not in dp.columns:\n",
    "            dp = dp.withColumn(field, dp['dp'][field].cast(meta[field]))\n",
    "\n",
    "    if item not in \"dataflows\":\n",
    "        pkfk_1 = f\"{item}id\"\n",
    "        dp = dp.withColumnRenamed(\"id\", pkfk_1 )\n",
    "        dp = dp.withColumn(\"UID\", concat(col(f\"{item}id\"), lit(\"-\"), col(\"ts_year\"), col(\"ts_month\"),col(\"ts_day\")))\n",
    "    else:\n",
    "        pkfk_1 = f\"{item}objectid\"\n",
    "        dp = dp.withColumnRenamed(\"objectid\", pkfk_1 )\n",
    "        dp = dp.withColumn(\"UID\", concat(col(f\"{item}objectid\"), lit(\"-\"), col(\"ts_year\"), col(\"ts_month\"),col(\"ts_day\")))\n",
    "\n",
    "\n",
    "    if save_df:\n",
    "        print(f\"saving...Files/{test}/{item}\")\n",
    "        dp.drop(\"dp\").write.format(\"parquet\").mode(\"overwrite\").save(f\"Files/{test}/{item}\")    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Working on item: {item}\")\n",
    "    dataframes = dict()\n",
    "\n",
    "\n",
    "    # make the sub dataframes of the parent\n",
    "    for array in df[0]:\n",
    "        id = \"id\"\n",
    "        if item in \"dataflows\":\n",
    "            id = \"objectId\"\n",
    "        elif array in \"tables\":\n",
    "            id = \"id\" # \"name\"\n",
    "        elif item == \"dataflows\" and array == \"relations\":            \n",
    "            id = \"objectId\"\n",
    "        elif item in \"dataflows\" and array in \"datasourceUsages\":\n",
    "            # id = \"datasourceInstanceId\"\n",
    "            id = \"objectId\"\n",
    "\n",
    "        print(f\"what array are we using:{array}\")\n",
    "       \n",
    "        dataframes[array] = dp.withColumn(array, explode(dp[\"dp\"][array]))\\\n",
    "                .withColumn(f\"{item}id\", dp[pkfk_1])\\\n",
    "                .select(pkfk_1,\"ts_year\", \"ts_month\", \"ts_day\", array)\n",
    "\n",
    "        dataframes[array] = dataframes[array].withColumn(f\"{item}_UID\",concat(col(pkfk_1),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\n",
    "        if array in [\"roles\",\"tables\"]:\n",
    "            print(\"i have entered the other side before\")\n",
    "            display(dataframes[array])\n",
    "            dataframes[array] = dataframes[array].withColumn(f\"{array}_UID\", concat(col(f\"{item}_UID\"),lit(\"-\"),dataframes[array][array][\"name\"]))\n",
    "            print(\"i have entered the other side after\")                  \n",
    "            display(dataframes[array])\n",
    "            \n",
    "        \n",
    "\n",
    "        # check for any arrays in the sub items\n",
    "        df2 = build_df(dataframes[array])\n",
    "        print(\"what is the extra\",df2[0])\n",
    "\n",
    "        if df2[0]:\n",
    "            dataframes2 = dict()\n",
    "\n",
    "            # display(dataframes[array])\n",
    "            for array2 in df2[0]:\n",
    "                \n",
    "                tt = dataframes[array].withColumn(array2, explode(dataframes[array][array][array2]))\n",
    "                tt = tt.withColumn(f\"{array}_UID\", concat(f\"{item}_UID\",lit(\"-\"),tt[array][\"name\"]))\n",
    "                tt = tt.drop(array)\n",
    "                tt = tt.select(flatten(tt.schema))     \n",
    "                # display(tt.limit(5))\n",
    "\n",
    "                # some of the children tables have the same name as the parent. The children columns\n",
    "                # will be appended with their index from the columns\n",
    "                dups = find_duplicates(tt.columns)\n",
    "                if dups:\n",
    "                    for k in dups:\n",
    "                        for i in range(1,len(dups[k])):\n",
    "                            idx = dups[k][i]\n",
    "\n",
    "                            # there isn't a way to do this with pyspark as renamed columns will in our case find the name of the column and rename it\n",
    "                            # but we have duplicate column names and this will rename both the columns and not the column at the indexed position\n",
    "                            # however, pandas does have that feature so we can rename a column at a specific index\n",
    "\n",
    "                            t = tt.toPandas()\n",
    "                            t.columns.values[idx]=f\"{array2}_{k}\"\n",
    "                            tt = spark.createDataFrame(t)\n",
    "\n",
    "                            if save_df:\n",
    "                                print(f\"saving...sub-sub items...Files/{test}/{item}/{array}/{array2}\")\n",
    "                                try:\n",
    "                                    # check to see if the folder exists\n",
    "                                    # if it does then append the files to the folder                            \n",
    "                                    item = mssparkutils.fs.ls(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                                    tt.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                                except Exception as e:\n",
    "                                    pass\n",
    "                                    tt.write.format(\"parquet\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                else:\n",
    "                    if save_df:\n",
    "                        print(f\"saving...sub sub-items...Files/{test}/{item}/{array}/{array2}\")\n",
    "                        try:\n",
    "                            # check to see if the folder exists\n",
    "                            # if it does then append the files to the folder                            \n",
    "                            item = mssparkutils.fs.ls(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                            tt.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                            tt.write.format(\"parquet\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "\n",
    "                del(tt)\n",
    "\n",
    "\n",
    "        # drop each sub dataframe from the parent\n",
    "        dp = dp.drop(array)\n",
    "\n",
    "\n",
    "    tmp = dict()\n",
    "\n",
    "    for key in dataframes:\n",
    "        print(f\"what is the key {key}\")\n",
    "        output = build_df(dataframes[key])\n",
    "        for field in output[1]:\n",
    "            if field not in dataframes[key].columns:\n",
    "                dataframes[key] = dataframes[key].withColumn(field, dataframes[key][key][field])\n",
    "\n",
    "        dataframes[key] = dataframes[key].drop(key)\n",
    "        if save_df:\n",
    "            print(f\"saving...sub items...Files/{test}/{item}/{key}\")\n",
    "            dataframes[key].write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{key}\")\n",
    "    \n",
    "    del(dataframes)\n",
    "    del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in mssparkutils.fs.ls(f\"Files/{nxt}\"):\n",
    "    if \"_SUCCESS\" in f.name or f.name.endswith(\"parquet\"):continue\n",
    "    try:\n",
    "        df = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}\")\n",
    "\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{f.name} Part 1 threw an error {e} for {f.name}\")\n",
    "\n",
    "    for ff in mssparkutils.fs.ls(f\"Files/{nxt}/{f.name}\"):\n",
    "        if \"_SUCCESS\" in ff.name or ff.name.endswith(\"parquet\"):continue\n",
    "        try:\n",
    "            dff = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}/{ff.name}\")\n",
    "\n",
    "            dff.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}_{ff.name}\")\n",
    "        except Exception as ee:\n",
    "            print(f\"{ff.name} threw an error {ee}\")\n",
    "\n",
    "        for fff in mssparkutils.fs.ls(f\"Files/{nxt}/{f.name}/{ff.name}\"):\n",
    "            if \"_SUCCESS\" in fff.name or fff.name.endswith(\"parquet\"):continue\n",
    "            try:\n",
    "                dfff = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}/{ff.name}/{fff.name}\")\n",
    "                dfff.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}_{ff.name}_{fff.name}\")\n",
    "            except Exception as eee:\n",
    "                print(f\"{fff.name} threw an error {eee}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = spark.read.option(\"multiline\",\"true\").json(\"Files/stage/datasetrefresh/*/*/*/*.json\").withColumn(\"file_Name\", input_file_name())\n",
    "\n",
    "dr = dr.withColumn(\"ts_year\", split(dr['file_Name'],\"/\")[7])\\\n",
    "    .withColumn(\"ts_month\", split(dr['file_Name'],\"/\")[8])\\\n",
    "    .withColumn(\"ts_day\", split(dr['file_Name'],\"/\")[9])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dr.select(\"dataset_id\",\"refreshAttempts\",\"ts_year\",\"ts_month\",\"ts_day\")\n",
    "r = r.withColumn(\"refreshAttemptsA\", explode(r[\"refreshAttempts\"]))\n",
    "\n",
    "r = r.withColumn(\"startTime\",r[\"refreshAttemptsA\"][\"startTime\"])\\\n",
    "    .withColumn(\"endTime\",r[\"refreshAttemptsA\"][\"endTime\"])\\\n",
    "    .withColumn(\"attemptId\",r[\"refreshAttemptsA\"][\"attemptId\"])\\\n",
    "    .withColumn(\"type\",r[\"refreshAttemptsA\"][\"type\"])\\\n",
    "    .withColumn(\"serviceExceptionJson\",r[\"refreshAttemptsA\"][\"serviceExceptionJson\"])\\\n",
    "    .withColumn(\"dataset_UID\", concat(col(\"dataset_id\"),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\\\n",
    "    .drop(\"refreshAttempts\",\"refreshAttemptsA\")\n",
    "\n",
    "r = r.withColumn(\"errorCode\", split(split(r[\"serviceExceptionJson\"],\",\")[0],\":\")[1])\\\n",
    "    .withColumn(\"errorDescription\", split(split(r[\"serviceExceptionJson\"],\",\")[1],\":\")[1])\\\n",
    "    .drop(\"serviceExceptionJson\")\n",
    "\n",
    "cols = [\"startTime\",\"endTime\"]\n",
    "\n",
    "for c in cols:\n",
    "    r = r.withColumn(f\"{c}_UTC\", from_utc_timestamp(c, \"UTC\"))\n",
    "\n",
    "\n",
    "r.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/datasetRefresh_attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "dr = dr.withColumn(\"dataset_UID\", concat(col(\"dataset_id\"),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\n",
    "dr = dr.drop(\"refreshAttempts\",\"serviceExceptionJson\")\n",
    "\n",
    "cols = [\"startTime\",\"endTime\"]\n",
    "\n",
    "for c in cols:\n",
    "    dr = dr.withColumn(f\"{c}_UTC\", from_utc_timestamp(c, \"UTC\"))\n",
    "\n",
    "dr = dr.withColumn(\"startDate\",date_format(col(\"startTime_UTC\"),\"yyyyMMdd\"))\\\n",
    "    .withColumn(\"startTime\", date_format(col(\"startTime_UTC\"), \"h:mm:ss a\"))\n",
    "\n",
    "dr.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/datasetRefresh\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
