{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import to_timestamp, from_utc_timestamp\n",
    "\n",
    "remove_tables=False\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobClient, BlobServiceClient\n",
    "from azure.identity import ClientSecretCredential\n",
    "\n",
    "\n",
    "\n",
    "def read_from_file(blob_name):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(os.getenv(\"STORAGE_ACCOUNT_CONNECTION_STRING\"))\n",
    "    blob_client = blob_service_client.get_blob_client(container=os.getenv(\"STORAGE_ACCOUNT_CONTAINER_NAME\"), blob=blob_name)\n",
    "    try:\n",
    "        blob_content = blob_client.download_blob().readall()\n",
    "        return blob_content \n",
    "    except Exception as e:\n",
    "        print(f\"Blob not found: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STORAGE_ACCOUNT_CONTAINER_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [blob.name \u001b[38;5;28;01mfor\u001b[39;00m blob \u001b[38;5;129;01min\u001b[39;00m blob_list]\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m blobs_in_path = list_blobs_in_path(\u001b[33m\"\u001b[39m\u001b[33mstage/catalog/scans/2025/07/15/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(blobs_in_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mlist_blobs_in_path\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_blobs_in_path\u001b[39m(path):\n\u001b[32m      2\u001b[39m     blob_service_client = BlobServiceClient.from_connection_string(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mSTORAGE_ACCOUNT_CONNECTION_STRING\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     container_client = blob_service_client.get_container_client(STORAGE_ACCOUNT_CONTAINER_NAME)\n\u001b[32m      4\u001b[39m     blob_list = container_client.list_blobs(name_starts_with=path)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [blob.name \u001b[38;5;28;01mfor\u001b[39;00m blob \u001b[38;5;129;01min\u001b[39;00m blob_list]\n",
      "\u001b[31mNameError\u001b[39m: name 'STORAGE_ACCOUNT_CONTAINER_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "def list_blobs_in_path(path):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(os.getenv(\"STORAGE_ACCOUNT_CONNECTION_STRING\"))\n",
    "    container_client = blob_service_client.get_container_client(os.getenv(\"STORAGE_ACCOUNT_CONTAINER_NAME\"))\n",
    "    blob_list = container_client.list_blobs(name_starts_with=path)\n",
    "    return [blob.name for blob in blob_list]\n",
    "\n",
    "# Example usage:\n",
    "blobs_in_path = list_blobs_in_path(\"stage/catalog/scans/2025/07/15/\")\n",
    "print(blobs_in_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blobs_in_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m blobs = read_from_file(blobs_in_path[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'blobs_in_path' is not defined"
     ]
    }
   ],
   "source": [
    "blobs = read_from_file(blobs_in_path[0])  # Assuming blobs_in_path is not empty and contains the path to the JSON file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Convert binary JSON data to string\n",
    "json_str = blobs.decode(\"utf-8\")\n",
    "# Parse JSON string to Python object\n",
    "json_obj = json.loads(json_str)\n",
    "\n",
    "# If the JSON is a list of records, create DataFrame directly\n",
    "if isinstance(json_obj, list):\n",
    "    df = spark.createDataFrame(json_obj)\n",
    "else:\n",
    "    # If it's a dict, wrap in a list\n",
    "    dfs = []\n",
    "    for workspace in json_obj[\"workspaces\"]:\n",
    "        df = pd.json_normalize([workspace])\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames in the list\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "final_df = final_df.rename(columns={\"id\": \"workspaceId\"})\n",
    "display(final_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "# Example: Flatten the 'datasets' column (which contains JSON/list data) into a new DataFrame\n",
    "# Replace 'datasets' with the column you want to flatten\n",
    "\n",
    "# Select the column with JSON/list data\n",
    "def flatten_json_column(df, json_col, parent_id_col):\n",
    "    \"\"\"\n",
    "    Flattens a column containing lists of dicts in a DataFrame and attaches the parent id column.\n",
    "    \"\"\"\n",
    "    if json_col in df.columns and parent_id_col in df.columns:\n",
    "        df = df.reset_index(drop=True)\n",
    "        exploded = df[[json_col, parent_id_col]].explode(json_col)\n",
    "        exploded = exploded[exploded[json_col].notna()]\n",
    "        if not exploded.empty:\n",
    "            normalized = pd.json_normalize(exploded[json_col])\n",
    "            normalized[parent_id_col] = exploded[parent_id_col].values\n",
    "            return normalized\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# # Example usage:\n",
    "# # normalized = flatten_json_column(final_df, json_col, 'workspaceId')\n",
    "# # display(normalized.head(2))\n",
    "# # Explode the column if it's a list of dicts\n",
    "# if json_col in final_df.columns:\n",
    "#     final_df = final_df.reset_index(drop=True)\n",
    "#     exploded = final_df[[json_col, 'workspaceId']].explode(json_col)\n",
    "#     exploded = exploded[exploded[json_col].notna()]\n",
    "#     if not exploded.empty:\n",
    "#         normalized = pd.json_normalize(exploded[json_col])\n",
    "#         normalized['workspaceId'] = exploded['workspaceId'].values\n",
    "#         display(normalized.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = flatten_json_column(final_df, 'datasets', 'workspaceId')\n",
    "tables = flatten_json_column(datasets, 'tables', 'id')\n",
    "display(tables.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "\n",
    "# List datasets in your workspace\n",
    "df_datasets = fabric.list_datasets()\n",
    "print(df_datasets)\n",
    "\n",
    "# Replace with your dataset name\n",
    "dataset_name = \"fill-in-Microsoft Fabric Capacity Metrics\"\n",
    "\n",
    "# List tables in the dataset\n",
    "df_tables = fabric.list_tables(dataset_name, include_columns=True)\n",
    "print(df_tables)\n",
    "\n",
    "df = fabric.read_table(dataset=dataset_name, table=\"MetricsByItemandOperationandDay\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spark.read.option(\"multiline\",\"true\").json(f\"Files/stage/catalog/scans/*/*/*/*.json\").withColumn(\"file_Name\", input_file_name())\n",
    "tmp = tmp.withColumn(\"ts_year\", split(tmp['file_Name'],\"/\")[8])\n",
    "tmp = tmp.withColumn(\"ts_month\", split(tmp['file_Name'],\"/\")[9])\n",
    "tmp = tmp.withColumn(\"ts_day\", split(tmp['file_Name'],\"/\")[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workspaces = tmp \\\n",
    "    .withColumn(\"workspaces\", explode(tmp[\"workspaces\"])) \\\n",
    "    .drop(tmp[\"datasourceInstances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_value(string):\n",
    "    start = string.find(':') + 1\n",
    "    end = string.find('(')\n",
    "    return string[start:end].strip()\n",
    "\n",
    "def extract_name(string):\n",
    "    start = string.find('--') + 1\n",
    "    end = string.find(':')\n",
    "    return string[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta(string):\n",
    "    start = string.find(':') + 1\n",
    "    end = string.find('(')\n",
    "    value = string[start:end].strip()\n",
    "\n",
    "    start_v = string.find('--') + 1\n",
    "    end_v = string.find(':')\n",
    "    key = string[start_v:end_v].split(\"-\")[-1].strip()    \n",
    "\n",
    "    meta = dict()\n",
    "    meta[key]=value\n",
    "\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(df):\n",
    "\n",
    "    lst = df._jdf.schema().treeString()\n",
    "    lines = lst.split(\"\\n\")\n",
    "\n",
    "    artifacts = list()\n",
    "    arrays = list()\n",
    "    root_columns = list()\n",
    "    fields = list()\n",
    "    meta = dict()\n",
    "\n",
    "    for line in lines:\n",
    "        x = line.split(sep=\"|\")\n",
    "        typ = extract_value(line)\n",
    "        name = extract_name(line).split(\"-\")[-1].strip()\n",
    "\n",
    "        if len(x)==2:\n",
    "            if typ not in ['struct','array']:\n",
    "                fields.append(name)\n",
    "                q = get_meta(line)\n",
    "                meta[name]=q[name]\n",
    "        if len(x)==3:\n",
    "            if typ not in ['struct','array']:\n",
    "                fields.append(name)\n",
    "                q = get_meta(line)\n",
    "                meta[name]=q[name]\n",
    "            else:\n",
    "                if typ == 'array':\n",
    "                    arrays.append(name)\n",
    "\n",
    "\n",
    "    return (arrays,fields,meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType  \n",
    "\n",
    "def flatten(schema, prefix=None):\n",
    "    fields = []\n",
    "    for field in schema.fields:\n",
    "        name = prefix + '.' + field.name if prefix else field.name\n",
    "        dtype = field.dataType\n",
    "        if isinstance(dtype, ArrayType):\n",
    "            dtype = dtype.elementType\n",
    "\n",
    "        if isinstance(dtype, StructType):\n",
    "            fields += flatten(dtype, prefix=name)\n",
    "        else:\n",
    "            fields.append(name)\n",
    "\n",
    "    return fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(lst):\n",
    "    duplicates = {}\n",
    "    for index, value in enumerate(lst):\n",
    "        if value in duplicates:\n",
    "            duplicates[value].append(index)\n",
    "        else:\n",
    "            duplicates[value] = [index]\n",
    "    return {key: value for key, value in duplicates.items() if len(value) > 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ( \n",
    "    StringType, BooleanType, IntegerType, FloatType, DateType, LongType, TimestampType, BinaryType, DoubleType \n",
    ") \n",
    "  \n",
    "\n",
    "dataTypes = dict(\n",
    "{\n",
    "    \"string\":\tStringType(),\t\n",
    "    \"integer\":\tIntegerType(),\t\n",
    "    \"long\":\t    LongType(),\t\n",
    "    \"float\":\tFloatType(),\t\n",
    "    \"double\":\tDoubleType(),\t\n",
    "    \"boolean\":\tBooleanType(),\t\n",
    "    \"date\":\t    DateType(),\t\n",
    "    \"timestamp\":TimestampType(),\n",
    "    \"binary\":\tBinaryType()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"bronze/catalog\"\n",
    "nxt = \"Silver/catalog\"\n",
    "save_df = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subitems = list()\n",
    "\n",
    "lst = df_workspaces._jdf.schema().treeString()\n",
    "\n",
    "lines = lst.split(\"\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    x = line.split(sep=\"|\")\n",
    "    typ = extract_value(line)\n",
    "    name = extract_name(line).split(\"-\")[-1].strip()\n",
    "\n",
    "    if len(x)==3:\n",
    "        if typ == \"array\":\n",
    "            subitems.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = df_workspaces.withColumn(\"datasets\", explode(df_workspaces[\"workspaces\"][\"datasets\"])).drop(\"workspaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "workspace = df_workspaces\n",
    "ww = build_df(workspace)\n",
    "\n",
    "for field in ww[1]:\n",
    "    if field not in [\"file_Name\",\"ts_year\",\"ts_month\",\"ts_day\"]:\n",
    "        workspace = workspace.withColumn(field, workspace['workspaces'][field])\n",
    "\n",
    "workspace = workspace.drop(\"workspaces\")\n",
    "workspace.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/workspaces\")\n",
    "\n",
    "for item in subitems:\n",
    "    # if item not in \"datasets\": continue\n",
    "    dp = df_workspaces.withColumn(\"dp\", explode(df_workspaces[\"workspaces\"][item]))\\\n",
    "        .withColumn(\"workspaceId\", df_workspaces[\"workspaces\"][\"id\"])\\\n",
    "        .drop(\"workspaces\") \n",
    "\n",
    "    df = build_df(dp)\n",
    "\n",
    "    for field in df[1]:\n",
    "        meta = df[2]\n",
    "        if field not in dp.columns:\n",
    "            dp = dp.withColumn(field, dp['dp'][field].cast(meta[field]))\n",
    "\n",
    "    if item not in \"dataflows\":\n",
    "        pkfk_1 = f\"{item}id\"\n",
    "        dp = dp.withColumnRenamed(\"id\", pkfk_1 )\n",
    "        dp = dp.withColumn(\"UID\", concat(col(f\"{item}id\"), lit(\"-\"), col(\"ts_year\"), col(\"ts_month\"),col(\"ts_day\")))\n",
    "    else:\n",
    "        pkfk_1 = f\"{item}objectid\"\n",
    "        dp = dp.withColumnRenamed(\"objectid\", pkfk_1 )\n",
    "        dp = dp.withColumn(\"UID\", concat(col(f\"{item}objectid\"), lit(\"-\"), col(\"ts_year\"), col(\"ts_month\"),col(\"ts_day\")))\n",
    "\n",
    "\n",
    "    if save_df:\n",
    "        print(f\"saving...Files/{test}/{item}\")\n",
    "        dp.drop(\"dp\").write.format(\"parquet\").mode(\"overwrite\").save(f\"Files/{test}/{item}\")    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Working on item: {item}\")\n",
    "    dataframes = dict()\n",
    "\n",
    "\n",
    "    # make the sub dataframes of the parent\n",
    "    for array in df[0]:\n",
    "        id = \"id\"\n",
    "        if item in \"dataflows\":\n",
    "            id = \"objectId\"\n",
    "        elif array in \"tables\":\n",
    "            id = \"id\" # \"name\"\n",
    "        elif item == \"dataflows\" and array == \"relations\":            \n",
    "            id = \"objectId\"\n",
    "        elif item in \"dataflows\" and array in \"datasourceUsages\":\n",
    "            # id = \"datasourceInstanceId\"\n",
    "            id = \"objectId\"\n",
    "\n",
    "        print(f\"what array are we using:{array}\")\n",
    "       \n",
    "        dataframes[array] = dp.withColumn(array, explode(dp[\"dp\"][array]))\\\n",
    "                .withColumn(f\"{item}id\", dp[pkfk_1])\\\n",
    "                .select(pkfk_1,\"ts_year\", \"ts_month\", \"ts_day\", array)\n",
    "\n",
    "        dataframes[array] = dataframes[array].withColumn(f\"{item}_UID\",concat(col(pkfk_1),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\n",
    "        if array in [\"roles\",\"tables\"]:\n",
    "            print(\"i have entered the other side before\")\n",
    "            display(dataframes[array])\n",
    "            dataframes[array] = dataframes[array].withColumn(f\"{array}_UID\", concat(col(f\"{item}_UID\"),lit(\"-\"),dataframes[array][array][\"name\"]))\n",
    "            print(\"i have entered the other side after\")                  \n",
    "            display(dataframes[array])\n",
    "            \n",
    "        \n",
    "\n",
    "        # check for any arrays in the sub items\n",
    "        df2 = build_df(dataframes[array])\n",
    "        print(\"what is the extra\",df2[0])\n",
    "\n",
    "        if df2[0]:\n",
    "            dataframes2 = dict()\n",
    "\n",
    "            # display(dataframes[array])\n",
    "            for array2 in df2[0]:\n",
    "                \n",
    "                tt = dataframes[array].withColumn(array2, explode(dataframes[array][array][array2]))\n",
    "                tt = tt.withColumn(f\"{array}_UID\", concat(f\"{item}_UID\",lit(\"-\"),tt[array][\"name\"]))\n",
    "                tt = tt.drop(array)\n",
    "                tt = tt.select(flatten(tt.schema))     \n",
    "                # display(tt.limit(5))\n",
    "\n",
    "                # some of the children tables have the same name as the parent. The children columns\n",
    "                # will be appended with their index from the columns\n",
    "                dups = find_duplicates(tt.columns)\n",
    "                if dups:\n",
    "                    for k in dups:\n",
    "                        for i in range(1,len(dups[k])):\n",
    "                            idx = dups[k][i]\n",
    "\n",
    "                            # there isn't a way to do this with pyspark as renamed columns will in our case find the name of the column and rename it\n",
    "                            # but we have duplicate column names and this will rename both the columns and not the column at the indexed position\n",
    "                            # however, pandas does have that feature so we can rename a column at a specific index\n",
    "\n",
    "                            t = tt.toPandas()\n",
    "                            t.columns.values[idx]=f\"{array2}_{k}\"\n",
    "                            tt = spark.createDataFrame(t)\n",
    "\n",
    "                            if save_df:\n",
    "                                print(f\"saving...sub-sub items...Files/{test}/{item}/{array}/{array2}\")\n",
    "                                try:\n",
    "                                    # check to see if the folder exists\n",
    "                                    # if it does then append the files to the folder                            \n",
    "                                    item = mssparkutils.fs.ls(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                                    tt.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                                except Exception as e:\n",
    "                                    pass\n",
    "                                    tt.write.format(\"parquet\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                else:\n",
    "                    if save_df:\n",
    "                        print(f\"saving...sub sub-items...Files/{test}/{item}/{array}/{array2}\")\n",
    "                        try:\n",
    "                            # check to see if the folder exists\n",
    "                            # if it does then append the files to the folder                            \n",
    "                            item = mssparkutils.fs.ls(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                            tt.write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "                        except Exception as e:\n",
    "                            pass\n",
    "                            tt.write.format(\"parquet\").save(f\"Files/{test}/{item}/{array}/{array2}\")\n",
    "\n",
    "                del(tt)\n",
    "\n",
    "\n",
    "        # drop each sub dataframe from the parent\n",
    "        dp = dp.drop(array)\n",
    "\n",
    "\n",
    "    tmp = dict()\n",
    "\n",
    "    for key in dataframes:\n",
    "        print(f\"what is the key {key}\")\n",
    "        output = build_df(dataframes[key])\n",
    "        for field in output[1]:\n",
    "            if field not in dataframes[key].columns:\n",
    "                dataframes[key] = dataframes[key].withColumn(field, dataframes[key][key][field])\n",
    "\n",
    "        dataframes[key] = dataframes[key].drop(key)\n",
    "        if save_df:\n",
    "            print(f\"saving...sub items...Files/{test}/{item}/{key}\")\n",
    "            dataframes[key].write.format(\"parquet\").mode(\"append\").save(f\"Files/{test}/{item}/{key}\")\n",
    "    \n",
    "    del(dataframes)\n",
    "    del(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in mssparkutils.fs.ls(f\"Files/{nxt}\"):\n",
    "    if \"_SUCCESS\" in f.name or f.name.endswith(\"parquet\"):continue\n",
    "    try:\n",
    "        df = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}\")\n",
    "\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{f.name} Part 1 threw an error {e} for {f.name}\")\n",
    "\n",
    "    for ff in mssparkutils.fs.ls(f\"Files/{nxt}/{f.name}\"):\n",
    "        if \"_SUCCESS\" in ff.name or ff.name.endswith(\"parquet\"):continue\n",
    "        try:\n",
    "            dff = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}/{ff.name}\")\n",
    "\n",
    "            dff.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}_{ff.name}\")\n",
    "        except Exception as ee:\n",
    "            print(f\"{ff.name} threw an error {ee}\")\n",
    "\n",
    "        for fff in mssparkutils.fs.ls(f\"Files/{nxt}/{f.name}/{ff.name}\"):\n",
    "            if \"_SUCCESS\" in fff.name or fff.name.endswith(\"parquet\"):continue\n",
    "            try:\n",
    "                dfff = spark.read.format(\"parquet\").load(f\"Files/{nxt}/{f.name}/{ff.name}/{fff.name}\")\n",
    "                dfff.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(f\"Tables/catalog_{f.name}_{ff.name}_{fff.name}\")\n",
    "            except Exception as eee:\n",
    "                print(f\"{fff.name} threw an error {eee}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = spark.read.option(\"multiline\",\"true\").json(\"Files/stage/datasetrefresh/*/*/*/*.json\").withColumn(\"file_Name\", input_file_name())\n",
    "\n",
    "dr = dr.withColumn(\"ts_year\", split(dr['file_Name'],\"/\")[7])\\\n",
    "    .withColumn(\"ts_month\", split(dr['file_Name'],\"/\")[8])\\\n",
    "    .withColumn(\"ts_day\", split(dr['file_Name'],\"/\")[9])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dr.select(\"dataset_id\",\"refreshAttempts\",\"ts_year\",\"ts_month\",\"ts_day\")\n",
    "r = r.withColumn(\"refreshAttemptsA\", explode(r[\"refreshAttempts\"]))\n",
    "\n",
    "r = r.withColumn(\"startTime\",r[\"refreshAttemptsA\"][\"startTime\"])\\\n",
    "    .withColumn(\"endTime\",r[\"refreshAttemptsA\"][\"endTime\"])\\\n",
    "    .withColumn(\"attemptId\",r[\"refreshAttemptsA\"][\"attemptId\"])\\\n",
    "    .withColumn(\"type\",r[\"refreshAttemptsA\"][\"type\"])\\\n",
    "    .withColumn(\"serviceExceptionJson\",r[\"refreshAttemptsA\"][\"serviceExceptionJson\"])\\\n",
    "    .withColumn(\"dataset_UID\", concat(col(\"dataset_id\"),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\\\n",
    "    .drop(\"refreshAttempts\",\"refreshAttemptsA\")\n",
    "\n",
    "r = r.withColumn(\"errorCode\", split(split(r[\"serviceExceptionJson\"],\",\")[0],\":\")[1])\\\n",
    "    .withColumn(\"errorDescription\", split(split(r[\"serviceExceptionJson\"],\",\")[1],\":\")[1])\\\n",
    "    .drop(\"serviceExceptionJson\")\n",
    "\n",
    "cols = [\"startTime\",\"endTime\"]\n",
    "\n",
    "for c in cols:\n",
    "    r = r.withColumn(f\"{c}_UTC\", from_utc_timestamp(c, \"UTC\"))\n",
    "\n",
    "\n",
    "r.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/datasetRefresh_attempts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "dr = dr.withColumn(\"dataset_UID\", concat(col(\"dataset_id\"),lit(\"-\"),col(\"ts_year\"),col(\"ts_month\"),col(\"ts_day\")))\n",
    "dr = dr.drop(\"refreshAttempts\",\"serviceExceptionJson\")\n",
    "\n",
    "cols = [\"startTime\",\"endTime\"]\n",
    "\n",
    "for c in cols:\n",
    "    dr = dr.withColumn(f\"{c}_UTC\", from_utc_timestamp(c, \"UTC\"))\n",
    "\n",
    "dr = dr.withColumn(\"startDate\",date_format(col(\"startTime_UTC\"),\"yyyyMMdd\"))\\\n",
    "    .withColumn(\"startTime\", date_format(col(\"startTime_UTC\"), \"h:mm:ss a\"))\n",
    "\n",
    "dr.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\",\"true\").save(\"Tables/datasetRefresh\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nbks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
